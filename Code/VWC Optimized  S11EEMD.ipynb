{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDDROVjXwpQL","executionInfo":{"status":"ok","timestamp":1681070587636,"user_tz":240,"elapsed":82565,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"2eb8caf7-b11b-48af-f338-0b69a61b178e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_0rHy8BV4PM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","\n","df_vwc = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd11_vwc.npy', allow_pickle=True)\n","df_stemp = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd11_stemp.npy', allow_pickle=True)\n","df_T = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_T.npy', allow_pickle=True)\n","df_RH = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_RH.npy', allow_pickle=True)\n","#df_DP = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_DP.npy', allow_pickle=True)\n","df_Rain = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_Rain.npy', allow_pickle=True)\n","df_WS = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_WS.npy', allow_pickle=True)\n","#df_WD = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_WD.npy', allow_pickle=True)\n","df_S = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_S.npy', allow_pickle=True)\n","\n","list_ = [df_vwc, df_Rain, df_stemp, df_T, df_RH,  df_WS, df_S]\n","\n","# IMF = np.empty((10, 10, 10))\n","IMFs = [[],[],[],[],[],[],[],[],[]]\n","# for i in list_:\n","#   print(i.shape)\n","for k in range(len(IMFs)):\n","  for i in list_:\n","    IMFs[k].append(i[k])\n","\n","\n","# df_vwc = pd.DataFrame(df_vwc).T\n","# df_vwc.columns = ['IMF1','IMF2','IMF3','IMF4','IMF5','IMF6','IMF7','IMF8','IMF9','IMF10','IMF11',\n","#               'IMF12','IMF13','IMF14', 'IMF15']\n","\n","# df_stemp = pd.DataFrame(df_stemp).T\n","# df_stemp.columns = ['IMF1','IMF2','IMF3','IMF4','IMF5','IMF6','IMF7','IMF8','IMF9','IMF10','IMF11',\n","#               'IMF12','IMF13']"]},{"cell_type":"code","source":["pip install optuna"],"metadata":{"id":"kHrWiY03JCc-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681070602703,"user_tz":240,"elapsed":6524,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"09bb08ae-97cd-4891-d27d-646769b346df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 KB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.65.0)\n","Collecting cmaes>=0.9.1\n","  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n","Collecting alembic>=1.5.0\n","  Downloading alembic-1.10.3-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.22.4)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.47)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n","Collecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n","Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.10.3 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5atfuoX2j7q"},"outputs":[],"source":["##LSTM\n","import pandas as pd\n","import numpy as np\n","import math\n","\n","import sklearn\n","import keras\n","import keras.backend as K\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import LSTM, CuDNNLSTM, MaxPooling1D, Conv1D, Flatten, BatchNormalization, Dropout, Input\n","from keras.models import Model\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.layers.convolutional import Conv1D, MaxPooling1D\n","from keras.optimizers import SGD\n","from keras.callbacks import EarlyStopping\n","\n","\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from math import sqrt\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from sklearn.preprocessing import MinMaxScaler\n","from multiprocessing import cpu_count \n","from joblib import Parallel \n","from joblib import delayed \n","from datetime import datetime\n","\n","import optuna \n","\n","from sklearn.preprocessing import StandardScaler\n","\n","from keras.utils import custom_object_scope\n","from keras.utils import get_custom_objects\n","\n","from keras.layers import LSTM, RepeatVector, TimeDistributed, BatchNormalization, Dropout\n","from keras.initializers import GlorotUniform\n","from keras.activations import relu\n","from joblib import Parallel, delayed\n","\n","import cProfile\n","\n","# from warnings import catch_warnings \n","# from warnings import filterwarnings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUXTQehjUj-o"},"outputs":[],"source":["####################################################################################################\n","def nmse(y_true, y_pred):\n","    \"\"\"\n","    Calculate the normalized mean squared error (NMSE) between y_true and y_pred.\n","    \"\"\"\n","    return np.mean(np.square(y_pred - y_true)) / np.var(y_true)\n","####################################################################################################\n","def smape(y_true, y_pred):\n","    \"\"\"\n","    Calculate the Symmetric Mean Absolute Percentage Error (SMAPE) between y_true and y_pred.\n","    \"\"\"\n","    return np.mean(2.0 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))) * 100\n","####################################################################################################\n","def smase(y_true, y_pred, y_naive):\n","    \"\"\"\n","    Calculate the Symmetric Mean Absolute Scaled Error (sMASE) between y_true and y_pred, relative \n","    to a symmetric naive forecast y_naive.\n","    \"\"\"\n","    # Calculate the symmetric naive forecast\n","    y_sym_naive = (y_naive.shift(1) + y_naive.shift(-1)) / 2.0\n","    \n","    # Calculate the MAPE of the forecast relative to the symmetric naive forecast\n","    mape_forecast = np.mean(np.abs(y_pred - y_sym_naive) / np.abs(y_true - y_sym_naive))\n","    \n","    # Calculate the MAPE of the historical data relative to the same symmetric naive forecast\n","    mape_historical = np.mean(np.abs(y_true - y_sym_naive.shift(1)) / np.abs(y_true - y_sym_naive))\n","    \n","    return mape_forecast / mape_historical\n","####################################################################################################\n","def nse(y_true, y_pred):\n","    \"\"\"\n","    Calculate the Nash-Sutcliffe Efficiency (NSE) between y_true and y_pred.\n","    \"\"\"\n","    mse = mean_squared_error(y_true, y_pred)\n","    rss = mse * len(y_true)\n","    tss = mean_absolute_error(y_true, np.mean(y_true)) ** 2 * len(y_true)\n","    return 1 - (rss / tss)\n","#####################################################################################################\n","def r2_keras(y_true, y_pred):\n","    SS_res =  K.sum(K.square(y_true - y_pred))\n","    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n","    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n","\n","# Register the custom metric function with Keras\n","with custom_object_scope({'r2_keras': r2_keras}):\n","    get_custom_objects().update({'r2_keras': r2_keras})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8rN9lieM2jQL"},"outputs":[],"source":["##Encoder/Decoder LSTM with Multivariate Input\n","\n","#Splitting datasets into train/test set\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[1:-6046], data[-6048:]\n","  #train, test = data[:-5817], data[-5817:-57]\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def rmse_(actual, predicted):\n","    return np.sqrt(mean_squared_error(actual, predicted))\n","\n","def nmse(actual, predicted):\n","    return mean_squared_error(actual, predicted) / np.var(actual)\n","\n","def smape(actual, predicted):\n","    return 100 / len(actual) * np.sum(2 * np.abs(predicted - actual) / (np.abs(actual) + np.abs(predicted)))\n","\n","def nse(actual, predicted):\n","    return 1 - np.sum((actual - predicted) ** 2) / np.sum((actual - np.mean(actual)) ** 2)\n","\n","def r2(y_true, y_pred):\n","    SS_res =  np.sum(K.square(y_true - y_pred))\n","    SS_tot =  np.sum(K.square(y_true - np.mean(y_true)))\n","    return 1 - SS_res/(SS_tot + np.finfo(float).eps) \n","\n","def compute_scores(actual, predicted):\n","    rmse_score = rmse_(actual, predicted)\n","    nmse_score = nmse(actual, predicted)\n","    smape_score = smape(actual, predicted)\n","    nse_score = nse(actual, predicted)\n","    r2_score = r2(actual, predicted)\n","    return rmse_score, nmse_score, smape_score, nse_score, r2_score\n","\n","def evaluate_forecasts(actual, predicted):\n","    # rmse_scores = [rmse_(actual[:, i],   predicted[:, i])      for i in  range(actual.shape[1])]\n","    # nmse_scores = [nmse(actual[:, i],    predicted[:, i])      for i in  range(actual.shape[1])]\n","    # smape_scores = [smape(actual[:, i],  predicted[:, i])      for i in  range(actual.shape[1])]\n","    # nse_scores = [nse(actual[:, i],      predicted[:, i])      for i in  range(actual.shape[1])]\n","    # r2_scores = [r2(actual[:,i],   predicted[:, i])      for i in  range(actual.shape[1])]\n","    # parallelize the loop using joblib\n","\n","    rmse_scores, nmse_scores, smape_scores, nse_scores, r2_scores = zip(*Parallel(n_jobs=-1)(\n","    delayed(compute_scores)(actual[:, i], predicted[:, i]) for i in range(actual.shape[1])))\n","\n","    overall_rmse = rmse_(actual, predicted.reshape(predicted.shape[0],predicted.shape[1]))\n","    overall_nmse = nmse(actual, predicted.reshape(predicted.shape[0],predicted.shape[1]))\n","    overall_smape = smape(actual, predicted.reshape(predicted.shape[0],predicted.shape[1]))\n","    overall_nse = nse(actual, predicted.reshape(predicted.shape[0],predicted.shape[1]))\n","    overall_r2 = r2(actual, predicted.reshape(predicted.shape[0],predicted.shape[1]))\n","    return overall_rmse,overall_nmse, overall_smape, overall_nse, overall_r2, rmse_scores, nmse_scores, smape_scores, nse_scores, r2_scores\n","\n","#####################################################################################################\n","#summarize scores\n","def summarize_scores(name, score, scores):\n","  s_scores = ', '.join(['%.1f' % s for s in scores])\n","  print('%s: [%.3f] %s' % (name, score, s_scores))\n","\n","# convert history into inputs and outputs\n","\n","\n","def to_supervised(train, n_input, n_out=144):\n","  #flattern data\n","  data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","  X, y = list(), list()\n","  in_start = 0\n","  # step over the entire history one time step at a time\n","  for i in range(len(data)):\n","    #define the end of the input sequence\n","    in_end = in_start + n_input\n","    out_end = in_end + n_out\n","    #ensure there is enough data\n","    if out_end <= len(data):\n","      X.append(data[in_start:in_end, :])\n","      y.append(data[in_end:out_end, 0])\n","    # move along one time step\n","    in_start += 1\n","  return np.array(X), np.array(y)\n"," \n","# #train the model LSTM-ED\n","\n","def build_model(train, config, verbose):\n","  # Unpack config\n","  n_input, n_nodes, n_epochs, n_batch = config\n","  # prepare data\n","  train_x, train_y = to_supervised(train, n_input)\n","  # Parameters\n","  n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n","  # reshape output into [samples, timesteps, features]\n","  train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n","  \n","  #define model\n","  \n","  # define model input\n","  inputs = Input(shape=(n_timesteps, n_features))\n","  \n","  # encoder layers\n","  encoder = CuDNNLSTM(n_nodes)(inputs)\n","  encoder = RepeatVector(n_outputs)(encoder)\n","  \n","  # decoder layers\n","  decoder = CuDNNLSTM(n_nodes, return_sequences=True)(encoder)\n","  decoder = TimeDistributed(Dense(n_nodes/2))(decoder)\n","  decoder = TimeDistributed(Dense(1))(decoder)\n","\n","  # define model\n","  model = Model(inputs=inputs, outputs=decoder)\n","\n","  model.compile(loss='mae', optimizer='adam',  metrics= ['MeanAbsoluteError', 'MeanSquaredError', 'mean_absolute_percentage_error', 'r2_keras'])\n","  \n","  # fit network\n","  model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose= verbose)\n","  plt.plot(model.history.history['loss'])\n","  plt.title('Training Metrics')\n","  plt.ylabel('Value')\n","  plt.xlabel('Epoch')\n","  plt.legend(['loss'], loc='upper right')\n","  plt.show()\n","  plt.clf()\n","  plt.plot(model.history.history['mean_absolute_error'])\n","  plt.plot(model.history.history['mean_squared_error'])\n","  plt.title('Training Metrics')\n","  plt.ylabel('Value')\n","  plt.xlabel('Epoch')\n","  plt.legend(['mean_absolute_error', 'mean_squared_error'], loc='upper right')\n","  plt.show()\n","  plt.clf()\n","  plt.plot(model.history.history['mean_absolute_percentage_error'])\n","  plt.title('Training Metrics')\n","  plt.ylabel('Value')\n","  plt.xlabel('Epoch')\n","  plt.legend(['mean_absolute_percentage_error'], loc='upper right')\n","  plt.show()\n","  plt.clf()\n","  plt.plot(model.history.history['r2_keras'])\n","  #plt.plot(model.history.history[])\n","  plt.title('Training Metrics')\n","  plt.ylabel('Value')\n","  plt.xlabel('Epoch')\n","  plt.legend(['r2_score'], loc='upper right')\n","  plt.show()\n","  return model\n","\n","#forecast\n","\n","def forecast(model, history, config):\n","  # unpack config\n","  n_input, _, _, _ = config\n","  #flattern data\n","  data = np.array(history)\n","  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n","  #retrieve last observations for input data\n","  input_x = data[-n_input:, :]\n","  # reshape into [1, n_input, n]\n","  input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n","  #forecast the next week\n","  yhat = model.predict(input_x, verbose=0)\n","  # we only want the vector forecast\n","  yhat = yhat\n","  return yhat\n","\n","def forecast_test(model, history, n_input):\n","  #flattern data\n","  data = np.array(history)\n","  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n","  #retrieve last observations for input data\n","  input_x = data[-n_input:, :]\n","  # reshape into [1, n_input, n]\n","  input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n","  #forecast the next week\n","  yhat = model.predict(input_x, verbose=0)\n","  # we only want the vector forecast\n","  yhat = yhat\n","  return yhat\n","\n","#evaluate a single model\n","\n","def evaluate_model(train, test, cfg, verbose):\n","  #fit model\n","  model= build_model(train, cfg, verbose)\n","  #test = X_scaler.transform(test)\n","  # history is a list of n_input data\n","  history = [x for x in train]\n","  #walk_forward validation over each week\n","  predictions = list()\n","  for i in range(len(test)):\n","    #predict the week\n","    yhat_sequence = forecast(model, history, cfg)[0]\n","    #store the predictions\n","    predictions.append(yhat_sequence)\n","    #get real observation an add to history for predicting the next week\n","    history.append(test[i, :])\n","  #evaluate predictions hours for each n_input\n","  predictions = np.array(predictions)\n","  #overall_rmse,overall_nmse, overall_smape, overall_nse, rmse_scores, nmse_scores, smape_scores, nse_scores\n","  overall_rmse,overall_nmse, overall_smape, overall_nse, overall_r2, rmse_scores, nmse_scores, smape_scores, nse_scores, r2_scores = evaluate_forecasts(test[:, :, 0], predictions)\n","  return overall_rmse,overall_nmse, overall_smape, overall_nse, overall_r2, rmse_scores, nmse_scores, smape_scores, nse_scores, r2_scores, predictions, model\n","\n","def evaluate_test(train, test, filename):\n","  model = load_model(filename)\n","  history = [x for x in test]\n","  #walk_forward validation over each week\n","  predictions = list()\n","  for i in range(len(test)):\n","    #predict the week\n","    yhat_sequence = forecast_test(model, history, n_input)[0]\n","    #store the predictions\n","    predictions.append(yhat_sequence)\n","    #get real observation an add to history for predicting the next week\n","    history.append(test[i, :])\n","    print(np.array(history).shape)\n","  #evaluate predictions hours for each n_input\n","  predictions = np.array(predictions)\n","  #overall_rmse,overall_nmse, overall_smape, overall_nse, rmse_scores, nmse_scores, smape_scores, nse_scores\n","  overall_rmse,overall_nmse, overall_smape, overall_nse, overall_r2, rmse_scores, nmse_scores, smape_scores, nse_scores, r2_scores = evaluate_forecasts(test[:, :, 0], predictions)\n","  return overall_rmse,overall_nmse, overall_smape, overall_nse, overall_r2, rmse_scores, nmse_scores, smape_scores, nse_scores, r2_scores, predictions\n","\n","\n","#Repeat evaluate\n","\n","def repeat_evaluate(train, test, config, verbose, n_repeats=2):\n","  # Convert config to a key\n","  key = str(config)\n","  # fit and evaluate the model n times\n","  overall_rmse,overall_nmse, overall_smape, overall_nse, overall_r2, rmse_scores, nmse_scores, smape_scores, nse_scores, r2_scores, predictions, model = evaluate_model(train, test, config, verbose)\n","  overall_rmse_models = [overall_rmse for _ in range(n_repeats)]\n","  overall_nmse_models = [overall_nmse for _ in range(n_repeats)]\n","  overall_smape_models = [overall_smape for _ in range(n_repeats)]\n","  overall_nse_models = [overall_nse for _ in range(n_repeats)]\n","  overall_r2_models = [overall_r2 for _ in range(n_repeats)]\n","  # Sumarize score\n","  result_rmse =  np.mean(overall_rmse_models)\n","  result_nmse =  np.mean(overall_nmse_models)\n","  result_smape = np.mean(overall_smape_models)\n","  result_nse =   np.mean(overall_nse_models)\n","  result_r2 =    np.mean(overall_r2_models)\n","  #print('> Model[%s] %.3f' % (key, result))\n","  return (key, result_rmse, result_nmse, result_smape, result_nse, result_r2), predictions\n","\n","# grid_search configs\n","\n","def grid_search(train, test, cfg_list, verbose):\n","  #evaluate configs\n","  pack = [repeat_evaluate(train, test, cfg, verbose) for cfg in cfg_list]\n","  #sort config by error, asc\n","  scores = [item[0] for item in pack]\n","  predictions = [item[1] for item in pack]\n","  scores.sort(key=lambda tup: tup[1])\n","  return scores, predictions\n","\n","\n","def model_configs():\n","  n_input = [200]\n","  n_nodes = [200]\n","  n_epochs = [250]\n","  n_batch = [10]\n","  configs = list()\n","  for i in n_input:\n","    for j in n_nodes:\n","      for k in n_epochs:\n","        for l in n_batch:\n","          cfg = [i, j, k , l]\n","          configs.append(cfg)\n","  print('Total configs: %d' % len(configs))\n","  return configs"]},{"cell_type":"markdown","metadata":{"id":"IMiW9mvEq-Ow"},"source":["IMF 0"]},{"cell_type":"code","source":["# define the objective function\n","def objective(trial):\n","\n","    # specify the hyperparameters to optimize\n","    n_input = trial.suggest_int('n_input', 1, 144)\n","    n_nodes = trial.suggest_int('n_nodes', 32, 256)\n","    n_epochs = trial.suggest_int('n_epochs', 10, 100)\n","    n_batch = trial.suggest_int('n_batch', 16, 128)\n","    \n","    # split the data into training and testing sets\n","    train, test = split_dataset(df_s.values)\n","    #train = df_s.values\n","\n","    # split the training data into folds for walk-forward validation\n","    n_splits = 5\n","    fold_size = int(len(train) / n_splits)\n","    folds = [train[i:i+fold_size] for i in range(0, len(train), fold_size)]\n","    \n","    # build the model using the hyperparameters\n","    model = build_model(train, (n_input, n_nodes, n_epochs, n_batch), verbose=False)\n","    \n","    \n","    # train and evaluate the model using walk-forward validation\n","    val_losses = []\n","    for i in range(n_splits):\n","        # prepare data for this fold\n","        train_x, train_y = to_supervised(np.concatenate(folds[:i] + folds[i+1:]), n_input)\n","        val_x, val_y = to_supervised(folds[i], n_input)\n","        train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n","        val_y = val_y.reshape((val_y.shape[0], val_y.shape[1], 1))\n","        \n","        # train the model using early stopping\n","        es = EarlyStopping(monitor='val_loss', patience=10)\n","        history = model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, validation_data=(val_x, val_y), verbose=False, callbacks=[es])\n","        \n","        # evaluate the model on the validation set for this fold\n","        val_loss = history.history['val_loss'][-1]\n","        val_losses.append(val_loss)\n","\n","        # metrics on validation set for this fold\n","        #al_mae = history.history['mean_absolute_error'][-1]\n","        #val_mse = history.history['mean_squared_error'][-1]\n","    \n","    # return the mean validation loss across all folds\n","    return np.mean(val_losses)"],"metadata":{"id":"3LTgWeb2IJ9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Scores_IMF = []\n","dataset = pd.read_csv('/content/drive/Shareddrives/1st Paper/2nd.csv', header = 0, infer_datetime_format=True, index_col=['Date Time'])\n","#dataset = dataset[:-8438]\n","# set the base name of the file\n","base_filename = '/content/drive/Shareddrives/1st Paper/Performance/S11_best_model_IMF_VWC'\n","\n","for m in range(0, 9):\n","\n","  sc = ([0, 0, 0, 0], 0, 0, 0, 0, 0)\n","  print('IMF: ', m)\n","  data = pd.DataFrame(IMFs[m]).T\n","  data.columns = ['S11_Top_VWC_Avg','Rain','S11_Top_Temp_Avg','Temp', 'Relative Humidity',\n","                 'Wind Speed', 'Solar Radiation']\n","  #testing = data[-8438:]\n","  #data = data[:-8438] \n","  data = data\n","\n","  %time\n","  External = data[['Rain','S11_Top_Temp_Avg','Temp', 'Relative Humidity',\n","                 'Wind Speed', 'Solar Radiation']]\n","  Predicted = data[['S11_Top_VWC_Avg']]\n","  index = pd.DataFrame(dataset.index)\n","\n","  scaler = StandardScaler()\n","  scaler.fit(External)\n","  print('Scale_:', scaler.scale_)\n","    \n","  var = pd.DataFrame(scaler.transform(External))\n","  \n","  #var = data\n","  df = pd.concat([index, data['S11_Top_VWC_Avg'], var], axis = 1)\n","  df = df.set_index('Date Time')\n","  df.columns = ['S11_Top_VWC_Avg','Rain','S11_Top_Temp_Avg','Temp', 'Relative Humidity',\n","                  'Wind Speed', 'Solar Radiation']\n","  df_s = df\n","\n","\n","  \n","  # create the study\n","  study = optuna.create_study(direction='minimize', storage='sqlite:///example.db')\n","\n","  # start the optimization\n","  study.optimize(objective, n_trials=2)\n","\n","  # retrieve the best hyperparameters\n","  best_params = study.best_params\n","  print('Best hyperparameters:', best_params)\n","\n","  train, test = split_dataset(df.values)\n","\n","  # train the model using the best hyperparameters on the entire training set\n","  model = build_model(train, (best_params['n_input'], best_params['n_nodes'], best_params['n_epochs'], best_params['n_batch']), verbose=True)\n","\n","  # set the filename based on the unique identifier\n","  filename = f\"{base_filename}_{m}.h5\"\n","\n","  # save the model to a file\n","  model.save(filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Wq9k2Z38VXUBe4Bi17pWHjnTfgb1hYoO"},"id":"bljnkECaDRUB","outputId":"93fd04a9-994b-43ab-da77-155ee24580f4","executionInfo":{"status":"ok","timestamp":1681083739699,"user_tz":240,"elapsed":12908948,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"colab":{"provenance":[{"file_id":"16LCV0vFu0dP4tIicRM-J1nWAh2Ih-DS7","timestamp":1680183871978},{"file_id":"1De9a8dY2WvIbo2Bl5XiXvYFK_QxJXa_p","timestamp":1678652875185},{"file_id":"1E56_GGGFK0hl-3IAM8MPA8fAU-mzCOJZ","timestamp":1678648817584},{"file_id":"1ZPuzOpeRUJy3E7NuHTJv5br3HkdDIiWV","timestamp":1678373010714},{"file_id":"1kGz386-OJdMAh4b-I5hd2hlpDymPKrjB","timestamp":1677165421867}],"mount_file_id":"16LCV0vFu0dP4tIicRM-J1nWAh2Ih-DS7","authorship_tag":"ABX9TyNpv7EYt3hqV7vQAT05tPRk"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}