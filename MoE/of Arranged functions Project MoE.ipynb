{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_WR6RvQpSeBw3JIE78DwJEbtzAFq0lte","timestamp":1682384663609},{"file_id":"1UeAwX0jGFbCRbfDoeN3AyQKP8LIaO0sa","timestamp":1682372831051}],"gpuType":"T4","mount_file_id":"1UeAwX0jGFbCRbfDoeN3AyQKP8LIaO0sa","authorship_tag":"ABX9TyNec1kbNCU03PM3abzs3Bbb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import math\n","\n","##LSTM\n","import pandas as pd\n","import numpy as np\n","import math\n","\n","import sklearn\n","import keras\n","import keras.backend as K\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import LSTM, CuDNNLSTM, MaxPooling1D, Conv1D, Flatten, BatchNormalization, Dropout, Input\n","from keras.models import Model\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.layers.convolutional import Conv1D, MaxPooling1D\n","from keras.optimizers import SGD\n","from keras.callbacks import EarlyStopping\n","\n","\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from math import sqrt\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from sklearn.preprocessing import MinMaxScaler\n","from multiprocessing import cpu_count\n","from joblib import Parallel\n","from joblib import delayed\n","from datetime import datetime\n","\n","#import optuna\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","from keras.utils import custom_object_scope\n","from keras.utils import get_custom_objects\n","from keras.models import load_model\n","\n","from keras.layers import LSTM, RepeatVector, TimeDistributed, BatchNormalization, Dropout\n","from keras.initializers import GlorotUniform\n","from keras.activations import relu\n","from joblib import Parallel, delayed\n","\n","import cProfile"],"metadata":{"id":"N13XmUTvQYOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_vwc = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_vwc_.npy', allow_pickle=True)\n","df_stemp = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_stemp_.npy', allow_pickle=True)\n","\n","df_vwc7 = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd7_vwc.npy', allow_pickle=True)\n","df_stemp7 = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd7_stemp.npy', allow_pickle=True)\n","\n","df_vwc11 = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd11_vwc.npy', allow_pickle=True)\n","df_stemp11 = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd11_stemp.npy', allow_pickle=True)\n","\n","df_T = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_T.npy', allow_pickle=True)\n","df_RH = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_RH.npy', allow_pickle=True)\n","#df_DP = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_DP.npy', allow_pickle=True)\n","df_Rain = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_Rain.npy', allow_pickle=True)\n","df_WS = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_WS.npy', allow_pickle=True)\n","#df_WD = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_WD.npy', allow_pickle=True)\n","df_S = np.load('/content/drive/Shareddrives/1st Paper/Array_eemd2_S.npy', allow_pickle=True)\n"],"metadata":{"id":"ENZTqB7pQcWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Scores_IMF = []\n","dataset = pd.read_csv('/content/drive/Shareddrives/1st Paper/2nd.csv', header = 0, infer_datetime_format=True, index_col=['Date Time'])\n","df = dataset[[\n","       'S2_Top_Temp_Avg', 'S2_Top_VWC_Avg'\n","\n","       ]]\n","#'Temp', 'Relative Humidity', 'Dew Point', 'Rain', 'Wind Speed', 'Wind Direction', 'Solar Radiation',"],"metadata":{"id":"V3nhoZKRonIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout\n","from keras.optimizers import Adam\n","import tensorflow_probability as tfp\n","\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.00009\n","\n","# Load the training data\n","train_data = np.array(df.head(100))\n","\n","\n","# Split the training data into input and output sequences\n","train_input = train_data[:, :]\n","print('train_input shape', train_input.shape)\n","train_output = train_data[:, -1:]\n","print('train_output shape', train_output.shape)\n","\n","# Define the experts\n","experts = []\n","for i in range(num_experts):\n","    expert_input = Input(shape=(input_dim,))\n","    expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","    expert_hidden = Dropout(0.2)(expert_hidden)\n","    expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","    expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)  # Add this line\n","    experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","# Define the gating network\n","gating_input = Input(shape=(input_dim,))\n","gating_hidden = gating_input\n","for i in range(len(gating_hidden_sizes)):\n","    gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","logits = gating_output[:, :num_experts]\n","params = gating_output[:, num_experts:]\n","params = tf.reshape(params, [-1, num_experts, 2])\n","\n","gating_distribution = tfp.distributions.MixtureSameFamily(\n","    mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","    components_distribution=tfp.distributions.Normal(\n","        loc=params[..., 0],\n","        scale=tf.math.softplus(params[..., 1])\n","    )\n",")\n","\n","gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","# Define the MoE model\n","inputs = Input(shape=(input_dim,))\n","outputs = []\n","for i in range(num_experts):\n","    expert_output = experts[i](inputs)\n","    outputs.append(expert_output)\n","\n","gating_output = gating_model(inputs)\n","weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","# Define the optimization algorithm\n","optimizer = Adam(lr=learning_rate)\n","\n","# Train the MoE model with the EM algorithm\n","for iteration in range(num_iterations):\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","      # Watch the trainable variables of the gating model\n","      tape.watch(gating_model.trainable_variables)\n","\n","      # Define the gating model and calculate the gating_loss\n","      gating_output = gating_model(gating_input)\n","      gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(gating_input))\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input))\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","# Make predictions on the test set using the MoE model\n","test_data = np.array(df[-100:])\n","test_input = test_data[:, :]\n","test_output = test_data[:, -1:]\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions)\n","print('Test loss = %.6f' % test_loss)\n"],"metadata":{"id":"5D37prgPMv4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout\n","from keras.optimizers import Adam\n","import tensorflow_probability as tfp\n","from keras.callbacks import EarlyStopping\n","\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","# Load the training data\n","train_data = np.array(df.head(144))\n","\n","\n","# Split the training data into input and output sequences\n","train_input = train_data[:, :]\n","print('train_input shape', train_input.shape)\n","train_output = train_data[:, -1:]\n","print('train_output shape', train_output.shape)\n","\n","# Define the experts\n","experts = []\n","for i in range(num_experts):\n","    expert_input = Input(shape=(input_dim,))\n","    expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","    expert_hidden = Dropout(0.2)(expert_hidden)\n","    expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","    expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)  # Add this line\n","    experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","# Define the gating network\n","gating_input = Input(shape=(input_dim,))\n","gating_hidden = gating_input\n","for i in range(len(gating_hidden_sizes)):\n","    gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","logits = gating_output[:, :num_experts]\n","params = gating_output[:, num_experts:]\n","params = tf.reshape(params, [-1, num_experts, 2])\n","\n","gating_distribution = tfp.distributions.MixtureSameFamily(\n","    mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","    components_distribution=tfp.distributions.Normal(\n","        loc=params[..., 0],\n","        scale=tf.math.softplus(params[..., 1])\n","    )\n",")\n","\n","gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","# Define the MoE model\n","inputs = Input(shape=(input_dim,))\n","outputs = []\n","for i in range(num_experts):\n","    expert_output = experts[i](inputs)\n","    outputs.append(expert_output)\n","\n","gating_output = gating_model(inputs)\n","weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","# Define the optimization algorithm\n","optimizer = Adam(lr=learning_rate)\n","\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, min_delta=0, baseline=None)\n","early_stopping.set_model(moe_model)  # Add this line\n","early_stopping.best = float('inf')  # Add this line\n","\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","      # Watch the trainable variables of the gating model\n","      tape.watch(gating_model.trainable_variables)\n","\n","      # Define the gating model and calculate the gating_loss\n","      gating_output = gating_model(gating_input)\n","      gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(gating_input))\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input))\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","    early_stopping.on_epoch_end(iteration, logs={'val_loss': train_loss})\n","\n","\n","    if early_stopping.stopped_epoch > 0:\n","      print('Early stopping triggered after iteration %d' % early_stopping.stopped_epoch)\n","      break\n","\n","    iteration += 1\n","\n","\n","# Make predictions on the test set using the MoE model\n","test_data = np.array(df[-144:])\n","test_input = test_data[:, :]\n","test_output = test_data[:, -1:]\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions)\n","print('Test loss = %.6f' % test_loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oh2AmkzsvnJO","executionInfo":{"status":"ok","timestamp":1682281600484,"user_tz":240,"elapsed":36441,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"cc527344-094c-46dd-fb1f-71832313f7dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_input shape (144, 2)\n","train_output shape (144, 1)\n","5/5 [==============================] - 0s 5ms/step\n","5/5 [==============================] - 0s 5ms/step\n","Iteration 1: Training loss = 9622792.000000\n","5/5 [==============================] - 0s 6ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 2: Training loss = 9477563.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 3: Training loss = 9321474.000000\n","5/5 [==============================] - 0s 4ms/step\n","5/5 [==============================] - 0s 5ms/step\n","Iteration 4: Training loss = 9151894.000000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 5: Training loss = 8968573.000000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 6: Training loss = 8772125.000000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 2ms/step\n","Iteration 7: Training loss = 8563696.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 8: Training loss = 8344747.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 9: Training loss = 8116824.000000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 10: Training loss = 7881578.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 11: Training loss = 7640504.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 2ms/step\n","Iteration 12: Training loss = 7395212.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 13: Training loss = 7147013.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 14: Training loss = 6897170.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 15: Training loss = 6646945.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 2ms/step\n","Iteration 16: Training loss = 6397350.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 17: Training loss = 6149163.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 18: Training loss = 5903274.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 19: Training loss = 5660381.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 20: Training loss = 5421078.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 21: Training loss = 5185888.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 22: Training loss = 4955285.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 23: Training loss = 4729669.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 24: Training loss = 4509344.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 25: Training loss = 4294452.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 26: Training loss = 4085200.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 27: Training loss = 3881824.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 28: Training loss = 3684529.500000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 29: Training loss = 3493410.250000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 30: Training loss = 3308537.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 31: Training loss = 3129869.750000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 32: Training loss = 2957352.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 33: Training loss = 2791005.750000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 34: Training loss = 2630798.750000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 35: Training loss = 2476635.750000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 36: Training loss = 2328381.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 37: Training loss = 2186079.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 38: Training loss = 2049672.625000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 2ms/step\n","Iteration 39: Training loss = 1919094.625000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 40: Training loss = 1799226.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 41: Training loss = 1690367.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 42: Training loss = 1591073.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 43: Training loss = 1500076.375000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 44: Training loss = 1416416.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 45: Training loss = 1343859.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 46: Training loss = 1280898.250000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 47: Training loss = 1226118.875000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 48: Training loss = 1178316.375000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 49: Training loss = 1136473.000000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 50: Training loss = 1099725.750000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 51: Training loss = 1067342.500000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 52: Training loss = 1038669.125000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 53: Training loss = 1013156.000000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 54: Training loss = 989966.875000\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 55: Training loss = 965330.000000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 56: Training loss = 939527.125000\n","5/5 [==============================] - 0s 4ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 57: Training loss = 912843.812500\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 58: Training loss = 885573.875000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 59: Training loss = 857632.125000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 60: Training loss = 826107.812500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 61: Training loss = 791516.312500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 62: Training loss = 754541.250000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 63: Training loss = 715877.750000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 64: Training loss = 676521.937500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 65: Training loss = 638910.687500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 66: Training loss = 603015.687500\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 67: Training loss = 568686.937500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 68: Training loss = 535841.312500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 4ms/step\n","Iteration 69: Training loss = 504744.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 70: Training loss = 477294.562500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 71: Training loss = 453029.343750\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 72: Training loss = 430823.500000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 73: Training loss = 409397.812500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 74: Training loss = 388763.312500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 75: Training loss = 368900.625000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 76: Training loss = 349676.937500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 77: Training loss = 329562.625000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 78: Training loss = 308635.125000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 79: Training loss = 288221.625000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 80: Training loss = 268402.062500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 81: Training loss = 249256.890625\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 82: Training loss = 230851.875000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 83: Training loss = 214291.140625\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 84: Training loss = 199407.437500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 85: Training loss = 185390.093750\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 86: Training loss = 172145.312500\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 87: Training loss = 159513.156250\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 88: Training loss = 146486.984375\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 89: Training loss = 133324.531250\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 90: Training loss = 120700.406250\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 91: Training loss = 108690.015625\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 92: Training loss = 98008.750000\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 93: Training loss = 88626.218750\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 94: Training loss = 80290.781250\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 95: Training loss = 72442.617188\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 96: Training loss = 64449.402344\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 97: Training loss = 56469.265625\n","5/5 [==============================] - 0s 3ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 98: Training loss = 48693.269531\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 99: Training loss = 41903.410156\n","5/5 [==============================] - 0s 2ms/step\n","5/5 [==============================] - 0s 3ms/step\n","Iteration 100: Training loss = 36102.890625\n","5/5 [==============================] - 0s 6ms/step\n","Test loss = 6050.879883\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, TimeDistributed\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_probability as tfp\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[0:-6047], data[-6048:]\n","  #train, test = data[:-5817], data[-5817:-57]\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def to_supervised(train, n_input):\n","    # Flatten data\n","    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","    X, y = list(), list()\n","    in_start = 0\n","    # Step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # Define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + 1\n","        # Ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data[in_start:in_end, :])\n","            y.append(data[in_end, 0])  # Modify this line to only include the first future time step\n","        # Move along one time step\n","        in_start += 1\n","    return np.array(X), np.array(y)\n","\n","\n","def build_moe_model(input_dim, output_dim, expert_hidden_sizes, expert_output_sizes,\n","                    gating_hidden_sizes, num_experts=3, learning_rate=0.0001,\n","                    num_iterations=100):\n","\n","    # Define the experts\n","    experts = []\n","    for i in range(num_experts):\n","        expert_input = Input(shape=(input_dim,))\n","        expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal', input_shape=(input_dim,))(expert_input)\n","        expert_hidden = Dropout(0.2)(expert_hidden)\n","        expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","        expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)\n","        experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 2])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        )\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        outputs.append(expert_output)\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    return moe_model, experts, gating_model\n","\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred, gating_output):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","\n","\n","train, test = split_dataset(df.values)\n","\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","#Train test split\n","train, test = split_dataset(df.values)\n","\n","# Input output\n","out, _ = to_supervised(train, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","train_input = train.reshape(train.shape[0]*train.shape[1], train.shape[2])[:-145,:]\n","train_output = out[:,:,1]\n","\n","\n","# Normalize input data\n","train_input = (train_input - np.mean(train_input, axis=0)) / np.std(train_input, axis=0)\n","\n","\n","moe_model, experts, gating_model = build_moe_model(input_dim, output_dim, expert_hidden_sizes,\n","                                                   expert_output_sizes, gating_hidden_sizes)\n","\n","# Define the optimization algorithm\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","# Learning rate scheduler\n","lr_scheduler = LearningRateScheduler(scheduler)\n","\n","# Train the MoE model with the EM algorithm\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","    optimizer.learning_rate.assign(current_learning_rate)\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","        # Watch the trainable variables of the gating model\n","        tape.watch(gating_model.trainable_variables)\n","\n","        # Define the gating model and calculate the gating_loss\n","        gating_output = gating_model(gating_input)\n","        gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","\n","\n","\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Stop training if the learning rate becomes too small\n","    if current_learning_rate < 1e-6:\n","        print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","        break\n","\n","    iteration += 1\n","\n","# Make predictions on the test set using the MoE model\n","\n","# Input output\n","out_test, _ = to_supervised(test, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","test_input = test.reshape(test.shape[0]*test.shape[1], test.shape[2])[:-145,:]\n","test_output = out_test[:,:,1]\n","\n","# Normalize test input data\n","test_input = (test_input - np.mean(test_input, axis=0)) / np.std(test_input, axis=0)\n","\n","\n","# Make predictions on the test set using the MoE model\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions, gating_model.predict(test_input))\n","\n","print('Test loss = %.6f' % test_loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDDn0TYM17mG","executionInfo":{"status":"ok","timestamp":1682345606595,"user_tz":240,"elapsed":255328,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"27bfe3d1-af79-43f9-960e-c2d3389c4f0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 1: Training loss = 107.021301\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 2: Training loss = 108.928482\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 3: Training loss = 111.087547\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 4: Training loss = 113.538475\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 5: Training loss = 116.306496\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 6: Training loss = 119.409637\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 7: Training loss = 122.862175\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 8: Training loss = 126.675789\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 9: Training loss = 130.858124\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 10: Training loss = 135.413849\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 11: Training loss = 140.365326\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 12: Training loss = 145.181198\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 13: Training loss = 149.827652\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 14: Training loss = 154.279434\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 15: Training loss = 158.517776\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 16: Training loss = 162.524155\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 17: Training loss = 166.292404\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 18: Training loss = 169.823151\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 19: Training loss = 173.119736\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 20: Training loss = 176.187546\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 21: Training loss = 179.037003\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 22: Training loss = 181.674103\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 23: Training loss = 184.111252\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 24: Training loss = 186.359558\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 25: Training loss = 188.426941\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 26: Training loss = 190.324066\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 27: Training loss = 192.062164\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 28: Training loss = 193.652481\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 29: Training loss = 195.105743\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 30: Training loss = 196.432312\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 31: Training loss = 197.642059\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 32: Training loss = 198.744400\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 33: Training loss = 199.748093\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 34: Training loss = 200.661377\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 35: Training loss = 201.491776\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 36: Training loss = 202.246521\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 37: Training loss = 202.932022\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 38: Training loss = 203.554367\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 39: Training loss = 204.119339\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 40: Training loss = 204.631729\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 41: Training loss = 205.096588\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 42: Training loss = 205.518097\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 43: Training loss = 205.899826\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 44: Training loss = 206.245499\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 45: Training loss = 206.558594\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 46: Training loss = 206.842331\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 47: Training loss = 207.099228\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 2s 3ms/step\n","Iteration 48: Training loss = 207.332031\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 49: Training loss = 207.542755\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 50: Training loss = 207.733475\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 51: Training loss = 207.906128\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 52: Training loss = 208.062332\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 53: Training loss = 208.203598\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 54: Training loss = 208.331528\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 55: Training loss = 208.447296\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 56: Training loss = 208.552017\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 57: Training loss = 208.646805\n","Learning rate dropped below 1e-6 after iteration 56\n","185/185 [==============================] - 0s 2ms/step\n","185/185 [==============================] - 0s 1ms/step\n","Test loss = 100.167702\n"]}]},{"cell_type":"code","source":["test_predictions_denormalized = test_predictions * np.std(train_output, axis=0) + np.mean(train_output, axis=0)"],"metadata":{"id":"HrvGTzpHbLHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Create a new figure object with a larger size\n","fig = plt.figure(figsize=(12, 8))\n","\n","# Create your plot within the new figure object\n","plt.plot(test_predictions_denormalized , color = 'red')\n","plt.plot(test_output, color = 'blue')\n","\n","# Display the plot\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"5Trc0qVK8s4R","executionInfo":{"status":"ok","timestamp":1682345658793,"user_tz":240,"elapsed":2463,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"57065ae3-15a9-412c-a35f-1ba8ad6c2c33"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"_BjWWYixo1-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, TimeDistributed\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_probability as tfp\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[0:-6047], data[-6048:]\n","  #train, test = data[:-5817], data[-5817:-57] 6048\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def to_supervised(train, n_input):\n","    # Flatten data\n","    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","    X, y = list(), list()\n","    in_start = 0\n","    # Step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # Define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + 1\n","        # Ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data[in_start:in_end, :])\n","            y.append(data[in_end, 0])  # Modify this line to only include the first future time step\n","        # Move along one time step\n","        in_start += 1\n","    return np.array(X), np.array(y)\n","\n","\n","def build_moe_model_with_autoencoder(input_dim, output_dim, expert_hidden_sizes, expert_output_sizes,\n","                                     gating_hidden_sizes, num_experts=3, learning_rate=0.0001,\n","                                     num_iterations=100):\n","\n","    # Define the experts\n","    experts = []\n","    for i in range(num_experts):\n","        if i == 0:  # Replace first expert with an autoencoder\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            encoded = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(encoded)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=encoded))  # Return encoded representation\n","        else:\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 2])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        )\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        if i == 0:  # For the autoencoder expert, append encoded representation to outputs list\n","            outputs.append(expert_output)\n","        else:\n","            outputs.append(experts[i](inputs))\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    return moe_model, experts, gating_model\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred, gating_output):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","#Data Split\n","train, test = split_dataset(df.values)\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","#Train test split\n","train, test = split_dataset(df.values)\n","\n","# Input output\n","out, _ = to_supervised(train, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","train_input = train.reshape(train.shape[0]*train.shape[1], train.shape[2])[:-145,:]\n","train_output = out[:,:,1]\n","\n","# Normalize input data\n","train_input = (train_input - np.mean(train_input, axis=0)) / np.std(train_input, axis=0)\n","\n","\n","#Build model\n","moe_model, experts, gating_model = build_moe_model(input_dim, output_dim, expert_hidden_sizes,\n","                                                   expert_output_sizes, gating_hidden_sizes)\n","\n","# Define the optimization algorithm\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","# Learning rate scheduler\n","lr_scheduler = LearningRateScheduler(scheduler)\n","\n","\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","    optimizer.learning_rate.assign(current_learning_rate)\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","        # Watch the trainable variables of the gating model\n","        tape.watch(gating_model.trainable_variables)\n","\n","        # Define the gating model and calculate the gating_loss\n","        gating_output = gating_model(gating_input)\n","        gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Stop training if the learning rate becomes too small\n","    if current_learning_rate < 1e-6:\n","        print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","        break\n","\n","    iteration += 1\n","\n","# Make predictions on the test set using the MoE model\n","\n","# Input output\n","out_test, _ = to_supervised(test, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","test_input = test.reshape(test.shape[0]*test.shape[1], test.shape[2])[:-145,:]\n","test_output = out_test[:,:,1]\n","\n","# Normalize test input data\n","test_input = (test_input - np.mean(test_input, axis=0)) / np.std(test_input, axis=0)\n","\n","\n","# Make predictions on the test set using the MoE model\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions, gating_model.predict(test_input))\n","\n","print('Test loss = %.6f' % test_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_vpvGConFto","executionInfo":{"status":"ok","timestamp":1682346792194,"user_tz":240,"elapsed":266110,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"85ecc472-19cb-4f08-c3b0-e8d22e69599b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 1: Training loss = 136.186615\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 2: Training loss = 136.962494\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 3: Training loss = 137.838974\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 4: Training loss = 138.833313\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 5: Training loss = 139.957062\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 6: Training loss = 141.225677\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 7: Training loss = 142.647842\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 3ms/step\n","Iteration 8: Training loss = 144.230988\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 9: Training loss = 145.981110\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 10: Training loss = 147.900925\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 11: Training loss = 150.020142\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 12: Training loss = 152.094452\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 13: Training loss = 154.104721\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 14: Training loss = 156.039703\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 15: Training loss = 157.890686\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 16: Training loss = 159.650116\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 2s 3ms/step\n","Iteration 17: Training loss = 161.313293\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 18: Training loss = 162.877579\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 19: Training loss = 164.342331\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 20: Training loss = 165.708527\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 21: Training loss = 166.978500\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 22: Training loss = 168.155655\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 23: Training loss = 169.243469\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 24: Training loss = 170.246674\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 25: Training loss = 171.169907\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 26: Training loss = 172.022110\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 27: Training loss = 172.804749\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 28: Training loss = 173.521225\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 29: Training loss = 174.175949\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 30: Training loss = 174.774414\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 31: Training loss = 175.320663\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 32: Training loss = 175.818939\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 33: Training loss = 176.272934\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 34: Training loss = 176.686371\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 35: Training loss = 177.062592\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 36: Training loss = 177.404800\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 37: Training loss = 177.715759\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 38: Training loss = 177.998367\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 39: Training loss = 178.255035\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 40: Training loss = 178.487946\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 41: Training loss = 178.699356\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 42: Training loss = 178.891022\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 43: Training loss = 179.064850\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 44: Training loss = 179.222366\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 45: Training loss = 179.365189\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 46: Training loss = 179.494583\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 47: Training loss = 179.611755\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 48: Training loss = 179.717911\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 49: Training loss = 179.814102\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 50: Training loss = 179.901108\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 51: Training loss = 179.979919\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 52: Training loss = 180.051163\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 53: Training loss = 180.115677\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 54: Training loss = 180.174088\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 55: Training loss = 180.226959\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 56: Training loss = 180.274780\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 57: Training loss = 180.318085\n","Learning rate dropped below 1e-6 after iteration 56\n","185/185 [==============================] - 0s 2ms/step\n","185/185 [==============================] - 0s 1ms/step\n","Test loss = 63.370300\n"]}]},{"cell_type":"code","source":["test_predictions_denormalized = test_predictions * np.std(train_output, axis=0) + np.mean(train_output, axis=0)"],"metadata":{"id":"CNphDN5VolTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Create a new figure object with a larger size\n","fig = plt.figure(figsize=(12, 8))\n","\n","# Create your plot within the new figure object\n","plt.plot(test_predictions_denormalized , color = 'red')\n","plt.plot(test_output, color = 'blue')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"9eEW0_-oome7","executionInfo":{"status":"ok","timestamp":1682346793082,"user_tz":240,"elapsed":913,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"e86dfbe4-9a38-4b12-9749-555294ba180d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Multiply, Add\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_probability as tfp\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[0:-6047], data[-6048:]\n","  #train, test = data[:-5817], data[-5817:-57] 6048\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def to_supervised(train, n_input):\n","    # Flatten data\n","    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","    X, y = list(), list()\n","    in_start = 0\n","    # Step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # Define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + 1\n","        # Ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data[in_start:in_end, :])\n","            y.append(data[in_end, 0])  # Modify this line to only include the first future time step\n","        # Move along one time step\n","        in_start += 1\n","    return np.array(X), np.array(y)\n","\n","def build_moe_model_with_autoencoder(input_dim, output_dim, expert_hidden_sizes, expert_output_sizes,\n","                                     gating_hidden_sizes, num_experts=3, learning_rate=0.0001,\n","                                     num_iterations=100):\n","\n","    # Define the experts\n","    experts = []\n","    for i in range(num_experts):\n","        if i == 0:  # Replace first expert with an autoencoder\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            encoded = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(encoded)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=encoded))  # Return encoded representation\n","        elif i == 1:  # Replace second expert with a CNN expert\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for CNN input\n","            expert_hidden = Conv1D(filters=32, kernel_size=3, activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = MaxPooling1D(pool_size=2)(expert_hidden)\n","            expert_hidden = Flatten()(expert_hidden)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","        else:  # Replace third expert with an attention-based model\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for LSTM input\n","            expert_hidden, _ = LSTM(expert_hidden_sizes[i], return_state=True, kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(expert_hidden_sizes[i], activation='tanh', kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(1, activation='softmax', kernel_initializer='he_normal')(attention)\n","            expert_hidden = Multiply()([expert_hidden, attention])\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 2])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        )\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        if i == 0:  # For the autoencoder expert, append encoded representation to outputs list\n","            outputs.append(expert_output)\n","        else:\n","            outputs.append(experts[i](inputs))\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    return moe_model, experts, gating_model\n","\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred, gating_output):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","train, test = split_dataset(df.values)\n","\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","#Train test split\n","train, test = split_dataset(df.values)\n","\n","# Input output\n","out, _ = to_supervised(train, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","train_input = train.reshape(train.shape[0]*train.shape[1], train.shape[2])[:-145,:]\n","train_output = out[:,:,1]\n","\n","# Normalize input data\n","train_input = (train_input - np.mean(train_input, axis=0)) / np.std(train_input, axis=0)\n","\n","#Build model\n","moe_model, experts, gating_model = build_moe_model(input_dim, output_dim, expert_hidden_sizes,\n","                                                   expert_output_sizes, gating_hidden_sizes)\n","\n","# Define the optimization algorithm\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","# Learning rate scheduler\n","lr_scheduler = LearningRateScheduler(scheduler)\n","\n","# Train the MoE model with the EM algorithm\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","    optimizer.learning_rate.assign(current_learning_rate)\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","        # Watch the trainable variables of the gating model\n","        tape.watch(gating_model.trainable_variables)\n","\n","        # Define the gating model and calculate the gating_loss\n","        gating_output = gating_model(gating_input)\n","        gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","\n","\n","\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Stop training if the learning rate becomes too small\n","    if current_learning_rate < 1e-6:\n","        print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","        break\n","\n","    iteration += 1\n","\n","# Input output\n","out_test, _ = to_supervised(test, 144)\n","\n","\n","# Reshape train_data so that the last column represents the output sequence\n","test_input = test.reshape(test.shape[0]*test.shape[1], test.shape[2])[:-145,:]\n","test_output = out_test[:,:,1]\n","\n","# Normalize test input data\n","test_input = (test_input - np.mean(test_input, axis=0)) / np.std(test_input, axis=0)\n","\n","# Make predictions on the test set using the MoE model\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions, gating_model.predict(test_input))\n","\n","print('Test loss = %.6f' % test_loss)\n","test_predictions_denormalized = test_predictions * np.std(train_output, axis=0) + np.mean(train_output, axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"163e2fe4-56ad-44a6-a1f7-72b37cf7bc1a","id":"njqXoSKtqxp4","executionInfo":{"status":"ok","timestamp":1682347183819,"user_tz":240,"elapsed":234447,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 1: Training loss = 94.362854\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 2: Training loss = 93.895279\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 3: Training loss = 93.393242\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 4: Training loss = 92.854828\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 5: Training loss = 92.283127\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 6: Training loss = 91.683960\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 7: Training loss = 91.064438\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 8: Training loss = 90.432556\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 9: Training loss = 89.796738\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 10: Training loss = 89.165092\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 11: Training loss = 88.556473\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 12: Training loss = 88.022850\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 13: Training loss = 87.557495\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 14: Training loss = 87.153870\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 15: Training loss = 86.806076\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 16: Training loss = 86.506325\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 17: Training loss = 86.248512\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 18: Training loss = 86.027206\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 19: Training loss = 85.836868\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 20: Training loss = 85.673790\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 3ms/step\n","Iteration 21: Training loss = 85.533920\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 22: Training loss = 85.413795\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 23: Training loss = 85.310753\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 24: Training loss = 85.222267\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 25: Training loss = 85.146217\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 26: Training loss = 85.080727\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 27: Training loss = 85.024231\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 28: Training loss = 84.975327\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 29: Training loss = 84.932922\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 30: Training loss = 84.896133\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 31: Training loss = 84.864159\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 32: Training loss = 84.836311\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 33: Training loss = 84.812019\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 34: Training loss = 84.790794\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 35: Training loss = 84.772156\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 36: Training loss = 84.755829\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 37: Training loss = 84.741470\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 38: Training loss = 84.728806\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 39: Training loss = 84.717636\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 40: Training loss = 84.707764\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 41: Training loss = 84.699028\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 42: Training loss = 84.691269\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 43: Training loss = 84.684372\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 44: Training loss = 84.678246\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 45: Training loss = 84.672791\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 46: Training loss = 84.667923\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 47: Training loss = 84.663574\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 48: Training loss = 84.659691\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 49: Training loss = 84.656227\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 50: Training loss = 84.653122\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 51: Training loss = 84.650330\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 52: Training loss = 84.647842\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 53: Training loss = 84.645592\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 54: Training loss = 84.643578\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 55: Training loss = 84.641762\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 56: Training loss = 84.640144\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 57: Training loss = 84.638672\n","Learning rate dropped below 1e-6 after iteration 56\n","185/185 [==============================] - 0s 2ms/step\n","185/185 [==============================] - 0s 1ms/step\n","Test loss = 51.700924\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Create a new figure object with a larger size\n","fig = plt.figure(figsize=(12, 8))\n","\n","# Create your plot within the new figure object\n","plt.plot(test_predictions_denormalized , color = 'red')\n","plt.plot(test_output, color = 'blue')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"-k9w5byfrMlX","executionInfo":{"status":"ok","timestamp":1682347210039,"user_tz":240,"elapsed":3351,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"ef58a03e-b970-41e7-d77e-2c53389d6b8f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"lericn38pkLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Multiply, Add\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_probability as tfp\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[0:-6047], data[-1440:]\n","  #train, test = data[:-5817], data[-5817:-57] 6048\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def to_supervised(train, n_input):\n","    # Flatten data\n","    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","    X, y = list(), list()\n","    in_start = 0\n","    # Step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # Define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + 1\n","        # Ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data[in_start:in_end, :])\n","            y.append(data[in_end, 0])  # Modify this line to only include the first future time step\n","        # Move along one time step\n","        in_start += 1\n","    return np.array(X), np.array(y)\n","\n","def build_moe_model_with_autoencoder(input_dim, output_dim, expert_hidden_sizes, expert_output_sizes,\n","                                     gating_hidden_sizes, num_experts=3, learning_rate=0.0001,\n","                                     num_iterations=100):\n","\n","    # Define the experts\n","    experts = []\n","    for i in range(num_experts):\n","        if i == 0:  # Replace first expert with an autoencoder\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            encoded = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(encoded)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=encoded))  # Return encoded representation\n","        elif i == 1:  # Replace second expert with a CNN expert\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for CNN input\n","            expert_hidden = Conv1D(filters=32, kernel_size=3, activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = MaxPooling1D(pool_size=2)(expert_hidden)\n","            expert_hidden = Flatten()(expert_hidden)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","        else:  # Replace third expert with an attention-based model\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for LSTM input\n","            expert_hidden, _ = LSTM(expert_hidden_sizes[i], return_state=True, kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(expert_hidden_sizes[i], activation='tanh', kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(1, activation='softmax', kernel_initializer='he_normal')(attention)\n","            expert_hidden = Multiply()([expert_hidden, attention])\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 3, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 3])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        ),\n","        weight_logits=params[..., 2]\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        if i == 0:  # For the autoencoder expert, append encoded representation to outputs list\n","            outputs.append(expert_output)\n","        else:\n","            outputs.append(experts[i](inputs))\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [gating_distribution.components_distribution[i].prob(expert_output) * gating_distribution.mixture_distribution.probs_parameter()[..., i, tf.newaxis] * expert_output for i, expert_output in enumerate(outputs)]\n","\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    return moe_model, experts, gating_model\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred, gating_output):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","\n","train, test = split_dataset(df.values)\n","\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","#Train test split\n","train, test = split_dataset(df.values)\n","\n","# Input output\n","out, _ = to_supervised(train, 144)\n","\n","# Reshape train_data so that the last column represents the output sequence\n","train_input = train.reshape(train.shape[0]*train.shape[1], train.shape[2])[:-145,:]\n","train_output = out[:,:,1]\n","\n","\n","\n","# Normalize input data\n","train_input = (train_input - np.mean(train_input, axis=0)) / np.std(train_input, axis=0)\n","\n","#Build model\n","moe_model, experts, gating_model = build_moe_model(input_dim, output_dim, expert_hidden_sizes,\n","                                                   expert_output_sizes, gating_hidden_sizes)\n","\n","# Define the optimization algorithm\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","# Learning rate scheduler\n","lr_scheduler = LearningRateScheduler(scheduler)\n","\n","\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","    optimizer.learning_rate.assign(current_learning_rate)\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","        # Watch the trainable variables of the gating model\n","        tape.watch(gating_model.trainable_variables)\n","\n","        # Define the gating model and calculate the gating_loss\n","        gating_output = gating_model(gating_input)\n","        gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Stop training if the learning rate becomes too small\n","    if current_learning_rate < 1e-6:\n","        print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","        break\n","\n","    iteration += 1\n","\n","# Make predictions on the test set using the MoE model\n","# Input output\n","out_test, _ = to_supervised(test, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","test_input = test.reshape(test.shape[0]*test.shape[1], test.shape[2])[:-145,:]\n","test_output = out_test[:,:,1]\n","\n","# Normalize test input data\n","test_input = (test_input - np.mean(test_input, axis=0)) / np.std(test_input, axis=0)\n","\n","# Make predictions on the test set using the MoE model\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions, gating_model.predict(test_input))\n","\n","print('Test loss = %.6f' % test_loss)\n","test_predictions_denormalized = test_predictions * np.std(train_output, axis=0) + np.mean(train_output, axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zBr_oP0A82GD","executionInfo":{"status":"ok","timestamp":1682354422730,"user_tz":240,"elapsed":238272,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"9ed6857c-7849-4b08-d830-fc57e7dd16c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 2s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 1: Training loss = 155.509521\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 2: Training loss = 154.122650\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 3: Training loss = 152.605347\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 4: Training loss = 150.953720\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 5: Training loss = 149.174622\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 6: Training loss = 147.280411\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 7: Training loss = 145.287231\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 8: Training loss = 143.213135\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 9: Training loss = 141.050446\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 10: Training loss = 138.836273\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 11: Training loss = 136.619034\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 12: Training loss = 134.603287\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 13: Training loss = 132.777328\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 14: Training loss = 131.128601\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 15: Training loss = 129.643417\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 16: Training loss = 128.309402\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 17: Training loss = 127.111618\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 18: Training loss = 126.036804\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 19: Training loss = 125.072868\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 20: Training loss = 124.208679\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 21: Training loss = 123.434013\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 22: Training loss = 122.743141\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 23: Training loss = 122.124329\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 24: Training loss = 121.569283\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 25: Training loss = 121.071266\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 26: Training loss = 120.624283\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 27: Training loss = 120.222916\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 28: Training loss = 119.862411\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 29: Training loss = 119.538490\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 30: Training loss = 119.247322\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 31: Training loss = 118.985489\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 32: Training loss = 118.749832\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 33: Training loss = 118.537697\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 34: Training loss = 118.346703\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 35: Training loss = 118.174721\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 36: Training loss = 118.019775\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 37: Training loss = 117.880135\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 38: Training loss = 117.754265\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 39: Training loss = 117.640785\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 40: Training loss = 117.538445\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 41: Training loss = 117.446106\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 42: Training loss = 117.362770\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 43: Training loss = 117.287628\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 44: Training loss = 117.219788\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 45: Training loss = 117.158562\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 46: Training loss = 117.103226\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 47: Training loss = 117.053307\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 48: Training loss = 117.008209\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 49: Training loss = 116.967453\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 50: Training loss = 116.930679\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 51: Training loss = 116.897469\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 52: Training loss = 116.867439\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 53: Training loss = 116.840324\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 54: Training loss = 116.815834\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 55: Training loss = 116.793694\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 56: Training loss = 116.773651\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 57: Training loss = 116.755600\n","Learning rate dropped below 1e-6 after iteration 56\n","41/41 [==============================] - 0s 2ms/step\n","41/41 [==============================] - 0s 1ms/step\n","Test loss = 51.157482\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Create a new figure object with a larger size\n","fig = plt.figure(figsize=(12, 8))\n","\n","# Create your plot within the new figure object\n","plt.plot(test_predictions_denormalized , color = 'red')\n","plt.plot(test_output, color = 'blue')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"owDS4s9H-f5M","executionInfo":{"status":"ok","timestamp":1682351874301,"user_tz":240,"elapsed":3864,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"9acc1f5f-3bdf-4d8c-8b44-0be12374ecd6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Multiply, Add\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_probability as tfp\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[0:-6047], data[-1440:]\n","  #train, test = data[:-5817], data[-5817:-57] 6048\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def to_supervised(train, n_input):\n","    # Flatten data\n","    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","    X, y = list(), list()\n","    in_start = 0\n","    # Step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # Define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + 1\n","        # Ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data[in_start:in_end, :])\n","            y.append(data[in_end, 0])  # Modify this line to only include the first future time step\n","        # Move along one time step\n","        in_start += 1\n","    return np.array(X), np.array(y)\n","\n","def build_moe_model_with_autoencoder(input_dim, output_dim, expert_hidden_sizes, expert_output_sizes,\n","                                     gating_hidden_sizes, num_experts=3, learning_rate=0.0001,\n","                                     num_iterations=100):\n","\n","    # Define the experts\n","    experts = []\n","    # Define the sizes of the hidden layers for each expert\n","    expert_hidden_sizes = [16, 32, 64]\n","\n","    # Define the sizes of the output layers for each expert\n","    expert_output_sizes = [144,144,144]\n","    for i in range(num_experts):\n","        if i == 0:  # Replace first expert with an autoencoder\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            encoded = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(encoded)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=encoded))  # Return encoded representation\n","        elif i == 1:  # Replace second expert with a CNN expert\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(dropout)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for CNN input\n","            expert_hidden = Conv1D(filters=32, kernel_size=3, activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = MaxPooling1D(pool_size=2)(expert_hidden)\n","            expert_hidden = Flatten()(expert_hidden)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","        else:  # Replace third expert with an attention-based model\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(dropout)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for LSTM input\n","            expert_hidden, _ = LSTM(expert_hidden_sizes[i], return_state=True, kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(expert_hidden_sizes[i], activation='tanh', kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(1, activation='softmax', kernel_initializer='he_normal')(attention)\n","            expert_hidden = Multiply()([expert_hidden, attention])\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 3, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 3])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        ),\n","        weight_logits=params[..., 2]\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        if i == 0:  # For the autoencoder expert, append encoded representation to outputs list\n","            outputs.append(expert_output)\n","        else:\n","            outputs.append(experts[i](inputs))\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [gating_distribution.components_distribution[i].prob(expert_output) * gating_distribution.mixture_distribution.probs_parameter()[..., i, tf.newaxis] * expert_output for i, expert_output in enumerate(outputs)]\n","\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    return moe_model, experts, gating_model\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred, gating_output):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n"],"metadata":{"id":"b2fgRc1m72v3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def objective(trial, train_input, train_output, input_dim, output_dim):\n","    # Define the hyperparameters to optimize using Optuna\n","    num_experts = trial.suggest_int(\"num_experts\", 2, 5)\n","    gating_hidden_sizes = [\n","        trial.suggest_int(f\"gating_hidden_size_{i}\", 16, 128) for i in range(2)\n","    ]\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n","\n","    dropout = trial.suggest_logouniform('Dropout', 0.1, 0.5)\n","\n","    # Build and train the model using the hyperparameters\n","    moe_model, experts, gating_model = build_moe_model_with_autoencoder(\n","        input_dim,\n","        output_dim,\n","        expert_hidden_sizes,\n","        expert_output_sizes,\n","        gating_hidden_sizes,\n","        num_experts=num_experts,\n","        learning_rate=learning_rate,\n","    )\n","\n","    # Train the MoE model with the EM algorithm\n","    iteration = 0\n","    while iteration < num_iterations:\n","\n","        # E step: Compute the responsibilities of each expert for each data point\n","        gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","        gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","        # M step: Update the parameters of each expert and the gating network\n","        for i in range(num_experts):\n","            expert_input = train_input\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","            with tf.GradientTape() as tape:\n","                # Watch the trainable variables of the expert model\n","                tape.watch(experts[i].trainable_variables)\n","\n","                # Define the expert model and calculate the expert_loss\n","                expert_output = experts[i](expert_input)\n","                expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","            # Compute the gradients\n","            expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","            # Clip gradients for expert models\n","            expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","            # Update the variables\n","            optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","        current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","        optimizer.learning_rate.assign(current_learning_rate)\n","\n","        gating_input = train_input\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the gating model\n","            tape.watch(gating_model.trainable_variables)\n","\n","            # Define the gating model and calculate the gating_loss\n","            gating_output = gating_model(gating_input)\n","            gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","\n","        # Compute the gradients\n","        gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","        # Clip gradients for the gating model\n","        gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","        # Evaluate the performance of the MoE model on the training set\n","        train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","        print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","        # Stop training if the learning rate becomes too small\n","        if current_learning_rate < 1e-6:\n","            print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","            break\n","\n","        iteration += 1\n","\n","    return train_loss\n"],"metadata":{"id":"Uewuo6RuKpVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input output\n","out, _ = to_supervised(train, 144)\n","\n","# Reshape train_data so that the last column represents the output sequence\n","train_input = train.reshape(train.shape[0]*train.shape[1], train.shape[2])[:-145,:]\n","train_output = out[:,:,1]\n","\n","# Normalize input data\n","train_input = (train_input - np.mean(train_input, axis=0)) / np.std(train_input, axis=0)\n","\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Create an Optuna study to optimize the hyperparameters\n","study = optuna.create_study(direction=\"minimize\")\n","\n","# Optimize the objective function using the train_input and train_output\n","study.optimize(partial(objective, train_input=train_input, train_output=train_output, input_dim=input_dim, output_dim=output_dim), n_trials=50)\n","\n","# Get the best hyperparameters from the optimization\n","best_params = study.best_params\n","print(\"Best hyperparameters: \", best_params)"],"metadata":{"id":"MYVb5cqs7Rzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model with the best hyperparameters\n","moe_model, experts, gating_model = build_moe_model_with_autoencoder(\n","    input_dim,\n","    output_dim,\n","    best_params[\"expert_hidden_sizes\"],\n","    best_params[\"expert_output_sizes\"],\n","    best_params[\"gating_hidden_sizes\"],\n","    num_experts=best_params[\"num_experts\"],\n","    learning_rate=best_params[\"learning_rate\"],\n",")\n","\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","    optimizer.learning_rate.assign(current_learning_rate)\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","        # Watch the trainable variables of the gating model\n","        tape.watch(gating_model.trainable_variables)\n","\n","        # Define the gating model and calculate the gating_loss\n","        gating_output = gating_model(gating_input)\n","        gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Stop training if the learning rate becomes too small\n","    if current_learning_rate < 1e-6:\n","        print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","        break\n","\n","    iteration += 1\n","\n","# Make predictions on the test set using the MoE model\n","# Input output\n","out_test, _ = to_supervised(test, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","test_input = test.reshape(test.shape[0]*test.shape[1], test.shape[2])[:-145,:]\n","test_output = out_test[:,:,1]\n","\n","# Normalize test input data\n","test_input = (test_input - np.mean(test_input, axis=0)) / np.std(test_input, axis=0)\n","\n","# Make predictions on the test set using the MoE model\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions, gating_model.predict(test_input))\n","\n","print('Test loss = %.6f' % test_loss)\n","test_predictions_denormalized = test_predictions * np.std(train_output, axis=0) + np.mean(train_output, axis=0)"],"metadata":{"id":"dUJsND_77bFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b1WMSW-17wt-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YNcEY7lY7w2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Multiply, Add\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_probability as tfp\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[0:-6047], data[-1440:]\n","  #train, test = data[:-5817], data[-5817:-57] 6048\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def to_supervised(train, n_input):\n","    # Flatten data\n","    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","    X, y = list(), list()\n","    in_start = 0\n","    # Step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # Define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + 1\n","        # Ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data[in_start:in_end, :])\n","            y.append(data[in_end, 0])  # Modify this line to only include the first future time step\n","        # Move along one time step\n","        in_start += 1\n","    return np.array(X), np.array(y)\n","\n","def build_moe_model_with_autoencoder(input_dim, output_dim, expert_hidden_sizes, expert_output_sizes,\n","                                     gating_hidden_sizes, num_experts=3, learning_rate=0.0001,\n","                                     num_iterations=100):\n","\n","    # Define the experts\n","    experts = []\n","    for i in range(num_experts):\n","        if i == 0:  # Replace first expert with an autoencoder\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            encoded = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(encoded)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=encoded))  # Return encoded representation\n","        elif i == 1:  # Replace second expert with a CNN expert\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for CNN input\n","            expert_hidden = Conv1D(filters=32, kernel_size=3, activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = MaxPooling1D(pool_size=2)(expert_hidden)\n","            expert_hidden = Flatten()(expert_hidden)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","        else:  # Replace third expert with an attention-based model\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_hidden = tf.expand_dims(expert_hidden, axis=1)  # Expand dimensions for LSTM input\n","            expert_hidden, _ = LSTM(expert_hidden_sizes[i], return_state=True, kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(expert_hidden_sizes[i], activation='tanh', kernel_initializer='he_normal')(expert_hidden)\n","            attention = Dense(1, activation='softmax', kernel_initializer='he_normal')(attention)\n","            expert_hidden = Multiply()([expert_hidden, attention])\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 3, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 3])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        ),\n","        weight_logits=params[..., 2]\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        if i == 0:  # For the autoencoder expert, append encoded representation to outputs list\n","            outputs.append(expert_output)\n","        else:\n","            outputs.append(experts[i](inputs))\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [gating_distribution.components_distribution[i].prob(expert_output) * gating_distribution.mixture_distribution.probs_parameter()[..., i, tf.newaxis] * expert_output for i, expert_output in enumerate(outputs)]\n","\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    return moe_model, experts, gating_model\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred, gating_output):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Replace MSE with NLL\n","    nll = -tf.reduce_sum(tf.math.log(gating_probabilities + 1e-8) * expert_losses, axis=-1)\n","    return tf.reduce_mean(nll)\n","\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","train, test = split_dataset(df.values)\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","#Train test split\n","train, test = split_dataset(df.values)\n","\n","# Input output\n","out, _ = to_supervised(train, 144)\n","\n","\n","# Reshape train_data so that the last column represents the output sequence\n","train_input = train.reshape(train.shape[0]*train.shape[1], train.shape[2])[:-145,:]\n","train_output = out[:,:,1]\n","\n","# Normalize input data\n","train_input = (train_input - np.mean(train_input, axis=0)) / np.std(train_input, axis=0)\n","\n","#Build model\n","moe_model, experts, gating_model = build_moe_model(input_dim, output_dim, expert_hidden_sizes,\n","                                                   expert_output_sizes, gating_hidden_sizes)\n","\n","# Define the optimization algorithm\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","# Learning rate scheduler\n","lr_scheduler = LearningRateScheduler(scheduler)\n","\n","# Train the MoE model with the EM algorithm\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","    optimizer.learning_rate.assign(current_learning_rate)\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","        # Watch the trainable variables of the gating model\n","        tape.watch(gating_model.trainable_variables)\n","\n","        # Define the gating model and calculate the gating_loss\n","        gating_output = gating_model(gating_input)\n","        gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Stop training if the learning rate becomes too small\n","    if current_learning_rate < 1e-6:\n","        print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","        break\n","\n","    iteration += 1\n","\n","# Input output\n","out_test, _ = to_supervised(test, 144)\n","\n","# Reshape train_data so that the last column represents the output sequence\n","test_input = test.reshape(test.shape[0]*test.shape[1], test.shape[2])[:-145,:]\n","test_output = out_test[:,:,1]\n","\n","# Normalize test input data\n","test_input = (test_input - np.mean(test_input, axis=0)) / np.std(test_input, axis=0)\n","\n","# Make predictions on the test set using the MoE model\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions, gating_model.predict(test_input))\n","\n","print('Test loss = %.6f' % test_loss)\n","test_predictions_denormalized = test_predictions * np.std(train_output, axis=0) + np.mean(train_output, axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAC9CGmYSLmu","executionInfo":{"status":"ok","timestamp":1682357636558,"user_tz":240,"elapsed":233044,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"c3a5e545-328e-49f0-dafe-3affdd525115"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 1: Training loss = 488.363586\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 2: Training loss = 498.211914\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 3: Training loss = 509.948212\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 4: Training loss = 524.086975\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 5: Training loss = 541.064819\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 6: Training loss = 561.288574\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 7: Training loss = 585.150146\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 8: Training loss = 613.019836\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 9: Training loss = 645.238403\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 10: Training loss = 682.093323\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 11: Training loss = 724.027222\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 12: Training loss = 766.502136\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 13: Training loss = 808.924072\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 14: Training loss = 850.782288\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 15: Training loss = 891.658752\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 16: Training loss = 931.242065\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 17: Training loss = 969.251831\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 18: Training loss = 1005.501221\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 19: Training loss = 1039.865356\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 20: Training loss = 1072.271606\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 21: Training loss = 1102.690430\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 22: Training loss = 1131.128906\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 23: Training loss = 1157.619141\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 24: Training loss = 1182.215942\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 25: Training loss = 1204.991089\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 26: Training loss = 1226.026367\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 27: Training loss = 1245.412109\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 28: Training loss = 1263.243774\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 29: Training loss = 1279.616821\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 30: Training loss = 1294.626099\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 31: Training loss = 1308.366333\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 32: Training loss = 1320.929321\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 33: Training loss = 1332.402954\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 34: Training loss = 1342.871216\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 35: Training loss = 1352.414062\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 36: Training loss = 1361.106934\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 37: Training loss = 1369.017212\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 38: Training loss = 1376.212280\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 39: Training loss = 1382.755493\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 40: Training loss = 1388.697754\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 41: Training loss = 1394.096191\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 42: Training loss = 1398.997070\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 43: Training loss = 1403.443359\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 44: Training loss = 1407.476807\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 45: Training loss = 1411.133667\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 46: Training loss = 1414.449463\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 47: Training loss = 1417.453979\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 48: Training loss = 1420.178223\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 49: Training loss = 1422.645752\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 50: Training loss = 1424.881226\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 51: Training loss = 1426.906982\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 52: Training loss = 1428.739624\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 53: Training loss = 1430.398682\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 54: Training loss = 1431.902466\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 55: Training loss = 1433.263062\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 56: Training loss = 1434.493652\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 3ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 57: Training loss = 1435.608032\n","Learning rate dropped below 1e-6 after iteration 56\n","41/41 [==============================] - 0s 3ms/step\n","41/41 [==============================] - 0s 1ms/step\n","Test loss = 204.840836\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Create a new figure object with a larger size\n","fig = plt.figure(figsize=(12, 8))\n","\n","# Create your plot within the new figure object\n","plt.plot(test_predictions_denormalized , color = 'red')\n","plt.plot(test_output, color = 'blue')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"0LlbJIk6VWWP","executionInfo":{"status":"ok","timestamp":1682357713270,"user_tz":240,"elapsed":1351,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"598949f8-0f50-48b5-df35-aa34f847de53"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x800 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, TimeDistributed\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow_probability as tfp\n","\n","def split_dataset(data):\n","  # split into standard weeks\n","  train, test = data[0:-6047], data[-432:]\n","  #train, test = data[:-5817], data[-5817:-57] 6048\n","  # restructure into windows of weekly data\n","  train = np.array(np.split(train, len(train)/144))\n","  test = np.array(np.split( test , len(test )/144))\n","  return train, test\n","\n","def to_supervised(train, n_input):\n","    # Flatten data\n","    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","    X, y = list(), list()\n","    in_start = 0\n","    # Step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # Define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + 1\n","        # Ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data[in_start:in_end, :])\n","            y.append(data[in_end, 0])  # Modify this line to only include the first future time step\n","        # Move along one time step\n","        in_start += 1\n","    return np.array(X), np.array(y)\n","\n","\n","train, test = split_dataset(df.values)\n","\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [144,144,144]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","#Train test split\n","train, test = split_dataset(df.values)\n","\n","# Input output\n","out, _ = to_supervised(train, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","train_input = train.reshape(train.shape[0]*train.shape[1], train.shape[2])[:-145,:]\n","train_output = out[:,:,1]\n","\n","\n","def build_moe_model_with_autoencoder(input_dim, output_dim, expert_hidden_sizes, expert_output_sizes,\n","                                     gating_hidden_sizes, num_experts=3, learning_rate=0.0001,\n","                                     num_iterations=100):\n","\n","    # Define the experts\n","    experts = []\n","    for i in range(num_experts):\n","        if i == 0:  # Replace first expert with an autoencoder\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            encoded = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(encoded)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_hidden)\n","            experts.append(Model(inputs=expert_input, outputs=encoded))  # Return encoded representation\n","        else:\n","            expert_input = Input(shape=(input_dim,))\n","            expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","            expert_hidden = Dropout(0.2)(expert_hidden)\n","            expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","            expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)\n","            experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 2])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        )\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        if i == 0:  # For the autoencoder expert, append encoded representation to outputs list\n","            outputs.append(expert_output)\n","        else:\n","            outputs.append(experts[i](inputs))\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    return moe_model, experts, gating_model\n","\n","\n","# Normalize input data\n","train_input = (train_input - np.mean(train_input, axis=0)) / np.std(train_input, axis=0)\n","\n","# Pad output sequences to the same length\n","\n","# train_output = pad_sequences(train_output, maxlen=max(expert_output_sizes), padding='post', dtype='float32')\n","\n","\n","moe_model, experts, gating_model = build_moe_model(input_dim, output_dim, expert_hidden_sizes,\n","                                                   expert_output_sizes, gating_hidden_sizes)\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred, gating_output):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","\n","\n","# Define the optimization algorithm\n","optimizer = Adam(learning_rate=learning_rate)\n","\n","# Learning rate scheduler\n","def scheduler(epoch, lr):\n","    if epoch < 10:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","\n","lr_scheduler = LearningRateScheduler(scheduler)\n","\n","# Train the MoE model with the EM algorithm\n","# Train the MoE model with the EM algorithm\n","iteration = 0\n","while iteration < num_iterations:\n","\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","    current_learning_rate = scheduler(iteration, optimizer.learning_rate.numpy())\n","    optimizer.learning_rate.assign(current_learning_rate)\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","        # Watch the trainable variables of the gating model\n","        tape.watch(gating_model.trainable_variables)\n","\n","        # Define the gating model and calculate the gating_loss\n","        gating_output = gating_model(gating_input)\n","        gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(train_input), gating_output)\n","\n","\n","\n","\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input), gating_model.predict(train_input))\n","\n","\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Stop training if the learning rate becomes too small\n","    if current_learning_rate < 1e-6:\n","        print('Learning rate dropped below 1e-6 after iteration %d' % iteration)\n","        break\n","\n","    iteration += 1\n","\n","# Make predictions on the test set using the MoE model\n","# test_data = np.array(df[-144:])\n","# test_input = test_data[:, :]\n","# test_output = test_data[:, -1]\n","# test_output = [[int(x)] for x in test_output]\n","\n","# Input output\n","out_test, _ = to_supervised(test, 144)\n","\n","# Load the training data\n","#train_data = np.array(df.head(17199))\n","\n","# Reshape train_data so that the last column represents the output sequence\n","test_input = test.reshape(test.shape[0]*test.shape[1], test.shape[2])[:-145,:]\n","test_output = out_test[:,:,1]\n","\n","# Normalize test input data\n","test_input = (test_input - np.mean(test_input, axis=0)) / np.std(test_input, axis=0)\n","\n","# # Pad test output sequences to the same length\n","# test_output_padded = []\n","# for seq in test_output:\n","#     padded_seq = seq[:1] + [seq[0]] * (max_output_len - 1)\n","#     test_output_padded.append(padded_seq)\n","# test_output = np.array(test_output_padded)\n","\n","# Make predictions on the test set using the MoE model\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions, gating_model.predict(test_input))\n","\n","print('Test loss = %.6f' % test_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7df24812-30ab-4781-cc41-e83bd1ea8773","id":"VC2iZrrspqXG"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 1: Training loss = 214.329041\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 2: Training loss = 212.905899\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 3: Training loss = 211.360413\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 4: Training loss = 209.667877\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 5: Training loss = 207.825867\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 6: Training loss = 205.839417\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 7: Training loss = 203.717682\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 8: Training loss = 201.471634\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 9: Training loss = 199.114120\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 10: Training loss = 196.658615\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 11: Training loss = 194.239990\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 12: Training loss = 191.988480\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 13: Training loss = 189.901779\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 14: Training loss = 187.974121\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 2s 4ms/step\n","Iteration 15: Training loss = 186.198822\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 16: Training loss = 184.568192\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 17: Training loss = 183.073761\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 18: Training loss = 181.706711\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 19: Training loss = 180.457703\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 20: Training loss = 179.317398\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 21: Training loss = 178.278351\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 22: Training loss = 177.333054\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 23: Training loss = 176.473557\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 24: Training loss = 175.692703\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 25: Training loss = 174.983734\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 26: Training loss = 174.340240\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 27: Training loss = 173.756546\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 28: Training loss = 173.227173\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 29: Training loss = 172.747375\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 30: Training loss = 172.312576\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 31: Training loss = 171.918716\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 32: Training loss = 171.561951\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 33: Training loss = 171.238876\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 34: Training loss = 170.946365\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 1ms/step\n","Iteration 35: Training loss = 170.681564\n","563/563 [==============================] - 1s 1ms/step\n","563/563 [==============================] - 1s 2ms/step\n","563/563 [==============================] - 1s 2ms/step\n","Iteration 36: Training loss = 170.441971\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 2s 4ms/step\n","563/563 [==============================] - 2s 3ms/step\n","Iteration 37: Training loss = 170.225082\n","177/563 [========>.....................] - ETA: 0s"]}]},{"cell_type":"code","source":["pip install optuna"],"metadata":{"id":"lgrL_At6pYwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna"],"metadata":{"id":"bzxFpVcNpb9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def objective(trial):\n","    # Define the input and output dimensions\n","    input_dim = 1\n","    output_dim = 1\n","\n","    # Suggest hyperparameters using Optuna\n","    num_experts = trial.suggest_int(\"num_experts\", 2, 5)\n","    expert_hidden_sizes = [\n","        trial.suggest_int(f\"expert_hidden_size_{i}\", 16, 64) for i in range(num_experts)\n","    ]\n","    expert_output_sizes = [\n","        trial.suggest_int(f\"expert_output_size_{i}\", 16, 64) for i in range(num_experts)\n","    ]\n","    gating_hidden_sizes = [\n","        trial.suggest_int(f\"gating_hidden_size_{i}\", 8, 32) for i in range(2)\n","    ]\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n","    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n","\n","    # Load the training data\n","    train_data = np.array(df.head(1000))\n","\n","    # Split the training data into input and output sequences\n","    train_input = train_data[:, :-1]\n","    print('train_input shape', train_input.shape)\n","    train_output = train_data[:, -1:]\n","    print('train_output shape', train_output.shape)\n","\n","    # Define the experts\n","    experts = []\n","    for i in range(num_experts):\n","        expert_input = Input(shape=(input_dim,))\n","        expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","        expert_hidden = Dropout(0.2)(expert_hidden)\n","        expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","        expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)  # Add this line\n","        experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","    # Define the gating network\n","    gating_input = Input(shape=(input_dim,))\n","    gating_hidden = gating_input\n","    for i in range(len(gating_hidden_sizes)):\n","        gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","    gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","    logits = gating_output[:, :num_experts]\n","    params = gating_output[:, num_experts:]\n","    params = tf.reshape(params, [-1, num_experts, 2])\n","\n","    gating_distribution = tfp.distributions.MixtureSameFamily(\n","        mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","        components_distribution=tfp.distributions.Normal(\n","            loc=params[..., 0],\n","            scale=tf.math.softplus(params[..., 1])\n","        )\n","    )\n","\n","    gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","    # Define the MoE model\n","    inputs = Input(shape=(input_dim,))\n","    outputs = []\n","    for i in range(num_experts):\n","        expert_output = experts[i](inputs)\n","        outputs.append(expert_output)\n","\n","    gating_output = gating_model(inputs)\n","    weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","    outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","    moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","    # Define the loss function\n","    def moe_loss(y_true, y_pred):\n","        y_true = tf.cast(y_true, y_pred.dtype)\n","        expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","        expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","        # Calculate the gating output using the provided inputs\n","        gating_output = gating_model(y_pred)\n","\n","        # Apply softmax to the logits to get probabilities\n","        gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","        print(\"y_true.shape: \", y_true.shape)\n","        print(\"y_pred.shape: \", y_pred.shape)\n","        print(\"expert_losses.shape: \", expert_losses.shape)\n","        print(\"gating_probabilities.shape: \", gating_probabilities.shape)\n","\n","        # Multiply expert_losses with the gating probabilities instead of logits\n","        gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","        return tf.reduce_mean(gating_losses)\n","\n","\n","    # Define the optimization algorithm\n","    optimizer = Adam(lr=learning_rate)\n","\n","    # Train the MoE model with the EM algorithm\n","    for iteration in range(num_iterations):\n","        # E step: Compute the responsibilities of each expert for each data point\n","        gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","        gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","\n","        # M step: Update the parameters of each expert and the gating network\n","        for i in range(num_experts):\n","            expert_input = train_input\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","            with tf.GradientTape() as tape:\n","                # Watch the trainable variables of the expert model\n","                tape.watch(experts[i].trainable_variables)\n","\n","                # Define the expert model and calculate the expert_loss\n","                expert_output = experts[i](expert_input)\n","                expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","            # Compute the gradients\n","            expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","            # Clip gradients for expert models\n","            expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","            # Update the variables\n","            for var, grad in zip(experts[i].trainable_variables, expert_gradient):\n","                var.assign_sub(learning_rate * grad)\n","\n","        gating_input = train_input\n","\n","        with tf.GradientTape() as tape:\n","          # Watch the trainable variables of the gating model\n","          tape.watch(gating_model.trainable_variables)\n","\n","          # Define the gating model and calculate the gating_loss\n","          gating_output = gating_model(gating_input)\n","          gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(gating_input))\n","\n","        # Compute the gradients\n","        gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","        # Clip gradients for the gating model\n","        gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","        # Update the variables\n","        for var, grad in zip(gating_model.trainable_variables, gating_gradient):\n","          var.assign_sub(learning_rate * grad)\n","\n","        # Evaluate the performance of the MoE model on the training set\n","        train_loss = moe_loss(train_output, moe_model.predict(train_input))\n","        print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","    # Make predictions on the test set using the MoE model\n","    test_data = np.array(df[-1000:])\n","    test_input = test_data[:, :-1]\n","    test_output = test_data[:, -1:]\n","    test_predictions = moe_model.predict(test_input)\n","\n","    test_loss = moe_loss(test_output, test_predictions)\n","    print('Test loss = %.6f' % test_loss)\n","\n","    return test_loss  # Return the test loss to be minimized"],"metadata":{"id":"w9tPi2VMQXzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=50)  # Set the number of trials you want to run\n","\n","# Print the best hyperparameters found\n","print(\"Best trial:\")\n","trial = study.best_trial\n","print(f\"Value: {trial.value}\")\n","print(\"Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")"],"metadata":{"id":"i4tMFuzNpUem"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWdBowZ4QAmZ"},"outputs":[],"source":["import numpy as np\n","from tensorflow import keras\n","from keras import layers\n","from sklearn.mixture import GaussianMixture\n","\n","# Generate example time series data\n","df = dataset[['S2_Top_VWC_Avg']]\n","data = np.array(df)\n","\n","# Define segment length\n","segment_length = 50\n","\n","# Segment the time series data\n","segments = np.array([data[i:i+segment_length] for i in range(0, len(data)-segment_length)])\n","\n","# Define the deep learning models\n","models = []\n","models.append(keras.Sequential([\n","    layers.Input(shape=(segment_length, 1)),\n","    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n","    layers.MaxPooling1D(pool_size=2),\n","    layers.Flatten(),\n","    layers.Dense(units=128, activation='relu'),\n","    layers.Dense(units=1, activation='linear')\n","]))\n","models.append(keras.Sequential([\n","    layers.Input(shape=(segment_length, 1)),\n","    layers.LSTM(units=64, return_sequences=True),\n","    layers.LSTM(units=32),\n","    layers.Dense(units=1, activation='linear')\n","]))\n","\n","# Compile the models\n","for model in models:\n","    model.compile(loss='mse', optimizer='adam')\n","\n","# Fit each model to each segment\n","for model in models:\n","    for i, segment in enumerate(segments):\n","        X = segment[:-1].reshape(-1, segment_length-1, 1)\n","        y = segment[1:].reshape(-1, 1)\n","        model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n","\n","# Fit a Gaussian mixture model to learn segment weights\n","gmm = GaussianMixture(n_components=len(segments), covariance_type='full')\n","gmm.fit(segments)\n","\n","# Use the segment weights to combine the predictions of the models\n","def predict(x):\n","    segment_index = gmm.predict(x.reshape(1, -1))[0]\n","    segment = segments[segment_index]\n","    model = models[segment_index % len(models)]\n","    X = segment[:-1].reshape(-1, segment_length-1, 1)\n","    y = segment[1:].reshape(-1, 1)\n","    model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n","    return model.predict(x.reshape(1, -1, 1))[0][0]\n","\n","# Make a forecast using the mixture of experts model\n","forecast = [predict(data[i:i+segment_length]) for i in range(len(data)-segment_length)]\n"]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout, LSTM\n","from keras.optimizers import Adam\n","import tensorflow_probability as tfp\n","from xgboost import XGBRegressor\n","\n","tf.config.experimental_run_functions_eagerly(True)\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 5\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64, 128]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [32, 32, 32, 128]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","# Load the training data\n","train_data = np.array(df.head(1000))\n","\n","# Split the training data into input and output sequences\n","train_input = train_data[:, :]\n","print('train_input shape', train_input.shape)\n","train_output = train_data[:, -1:]\n","print('train_output shape', train_output.shape)\n","\n","# Reshape the training input for the LSTM expert\n","train_input_lstm = np.reshape(train_input, (train_input.shape[0], train_input.shape[1], 1))\n","\n","# Define the experts\n","experts = []\n","for i in range(4):\n","    expert_input = Input(shape=(input_dim,))\n","    expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","    expert_hidden = Dropout(0.2)(expert_hidden)\n","    expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","    expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)  # Add this line\n","    experts.append(Model(inputs=expert_input, outputs=expert_output))\n","\n","# Define the LSTM expert\n","lstm_expert_input = Input(shape=(input_dim, 1))\n","lstm_expert_hidden = LSTM(64, activation='relu', return_sequences=True)(lstm_expert_input)\n","lstm_expert_hidden = LSTM(32, activation='relu', return_sequences=False)(lstm_expert_hidden)\n","lstm_expert_output = Dense(output_dim, activation='linear')(lstm_expert_hidden)\n","lstm_expert = Model(inputs=lstm_expert_input, outputs=lstm_expert_output)\n","\n","# Add the LSTM expert to the list of experts\n","experts.append(lstm_expert)\n","\n","# Define the gating network\n","gating_input = Input(shape=(input_dim,))\n","gating_hidden = gating_input\n","for i in range(len(gating_hidden_sizes)):\n","    gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","logits = gating_output[:, :num_experts]\n","params = gating_output[:, num_experts:]\n","params = tf.reshape(params, [-1, num_experts, 2])\n","\n","gating_distribution = tfp.distributions.MixtureSameFamily(\n","    mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","    components_distribution=tfp.distributions.Normal(\n","        loc=params[..., 0],\n","        scale=tf.math.softplus(params[..., 1])\n","    )\n",")\n","\n","gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","# Define the MoE model\n","inputs = Input(shape=(input_dim,))\n","outputs = []\n","for i in range(4):\n","    expert_output = experts[i](inputs)\n","    outputs.append(expert_output)\n","\n","# Add LSTM expert output to the list of outputs\n","lstm_outputs = experts[4](tf.expand_dims(inputs, axis=-1))\n","outputs.append(lstm_outputs)\n","\n","gating_output = gating_model(inputs)\n","weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","\n","# Define the optimization algorithm\n","optimizer = Adam(lr=learning_rate)\n","\n","# Train the MoE model with the EM algorithm\n","for iteration in range(num_iterations):\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","      # Watch the trainable variables of the gating model\n","      tape.watch(gating_model.trainable_variables)\n","\n","      # Define the gating model and calculate the gating_loss\n","      gating_output = gating_model(gating_input)\n","      gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(gating_input))\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input))\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","# Make predictions on the test set using the MoE model\n","test_data = np.array(df[-1000:])\n","test_input = test_data[:, :]\n","test_output = test_data[:, -1:]\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions)\n","print('Test loss = %.6f' % test_loss)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682204717619,"user_tz":240,"elapsed":260336,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"a495d22a-ebf6-4704-f47a-f2b830bb5fe1","id":"paHkGxxIJxQ1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_input shape (1000, 2)\n","train_output shape (1000, 1)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["32/32 [==============================] - 0s 4ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n","/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["32/32 [==============================] - 1s 43ms/step\n","Iteration 1: Training loss = 1024112.062500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 2: Training loss = 980264.187500\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 2s 62ms/step\n","Iteration 3: Training loss = 930691.437500\n","32/32 [==============================] - 1s 17ms/step\n","32/32 [==============================] - 3s 96ms/step\n","Iteration 4: Training loss = 875062.937500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 39ms/step\n","Iteration 5: Training loss = 813989.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 6: Training loss = 748527.312500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 57ms/step\n","Iteration 7: Training loss = 679969.250000\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 8: Training loss = 609678.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 9: Training loss = 539045.750000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 10: Training loss = 469404.281250\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 11: Training loss = 401968.562500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 53ms/step\n","Iteration 12: Training loss = 337772.031250\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 2s 57ms/step\n","Iteration 13: Training loss = 277764.031250\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 14: Training loss = 222766.562500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 15: Training loss = 173481.484375\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 16: Training loss = 130603.507812\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 40ms/step\n","Iteration 17: Training loss = 94822.679688\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 2s 57ms/step\n","Iteration 18: Training loss = 66542.148438\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 19: Training loss = 45171.640625\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 20: Training loss = 30356.953125\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 21: Training loss = 21764.251953\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 43ms/step\n","Iteration 22: Training loss = 19079.000000\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 2s 58ms/step\n","Iteration 23: Training loss = 21863.746094\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 24: Training loss = 28502.658203\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 25: Training loss = 37294.433594\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 26: Training loss = 47002.812500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 27: Training loss = 56760.972656\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 48ms/step\n","Iteration 28: Training loss = 65996.820312\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 2s 59ms/step\n","Iteration 29: Training loss = 74345.421875\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 30: Training loss = 81762.531250\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 31: Training loss = 88373.507812\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 32: Training loss = 94216.554688\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 45ms/step\n","Iteration 33: Training loss = 99322.210938\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 2s 48ms/step\n","Iteration 34: Training loss = 103790.265625\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 40ms/step\n","Iteration 35: Training loss = 107724.421875\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 36: Training loss = 111166.734375\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 37: Training loss = 114314.101562\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 46ms/step\n","Iteration 38: Training loss = 120561.882812\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 2s 48ms/step\n","Iteration 39: Training loss = 129907.101562\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 40: Training loss = 142355.843750\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 54ms/step\n","Iteration 41: Training loss = 157981.718750\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 40ms/step\n","Iteration 42: Training loss = 176796.421875\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 43: Training loss = 198692.890625\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 44: Training loss = 223484.828125\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 43ms/step\n","Iteration 45: Training loss = 251206.515625\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 57ms/step\n","Iteration 46: Training loss = 281830.468750\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 47: Training loss = 314641.343750\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 48: Training loss = 344112.093750\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 49: Training loss = 370420.250000\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 50: Training loss = 393557.406250\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 59ms/step\n","Iteration 51: Training loss = 413923.156250\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 52: Training loss = 431701.250000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 43ms/step\n","Iteration 53: Training loss = 446982.625000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 54: Training loss = 459797.093750\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 55: Training loss = 477646.281250\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 59ms/step\n","Iteration 56: Training loss = 500599.812500\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 57: Training loss = 528569.062500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 43ms/step\n","Iteration 58: Training loss = 561273.500000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 59: Training loss = 598034.562500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 60: Training loss = 638444.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 58ms/step\n","Iteration 61: Training loss = 671649.500000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 62: Training loss = 696332.312500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 63: Training loss = 713020.187500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 64: Training loss = 722645.500000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 65: Training loss = 731869.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 59ms/step\n","Iteration 66: Training loss = 744463.500000\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 2s 48ms/step\n","Iteration 67: Training loss = 760432.250000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 68: Training loss = 779455.437500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 69: Training loss = 791865.875000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 70: Training loss = 798040.500000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 71: Training loss = 798590.312500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 54ms/step\n","Iteration 72: Training loss = 803730.437500\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 73: Training loss = 812694.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 40ms/step\n","Iteration 74: Training loss = 824935.062500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 75: Training loss = 830545.812500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 76: Training loss = 831970.937500\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 2s 58ms/step\n","Iteration 77: Training loss = 831095.812500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 43ms/step\n","Iteration 78: Training loss = 837793.000000\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 79: Training loss = 851598.937500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 80: Training loss = 871987.437500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 43ms/step\n","Iteration 81: Training loss = 888730.875000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 49ms/step\n","Iteration 82: Training loss = 902087.062500\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 2s 46ms/step\n","Iteration 83: Training loss = 912195.812500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 84: Training loss = 928205.812500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 85: Training loss = 949504.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 86: Training loss = 973860.562500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 87: Training loss = 989970.937500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 57ms/step\n","Iteration 88: Training loss = 998500.875000\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 2s 51ms/step\n","Iteration 89: Training loss = 1000430.812500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 90: Training loss = 1006408.875000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 91: Training loss = 1016196.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 42ms/step\n","Iteration 92: Training loss = 1029536.375000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 93: Training loss = 1037321.437500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 2s 49ms/step\n","Iteration 94: Training loss = 1039736.312500\n","32/32 [==============================] - 0s 5ms/step\n","32/32 [==============================] - 2s 57ms/step\n","Iteration 95: Training loss = 1035748.187500\n","32/32 [==============================] - 0s 6ms/step\n","32/32 [==============================] - 1s 43ms/step\n","Iteration 96: Training loss = 1035381.937500\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 97: Training loss = 1038211.625000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 40ms/step\n","Iteration 98: Training loss = 1043935.625000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 99: Training loss = 1041796.250000\n","32/32 [==============================] - 0s 4ms/step\n","32/32 [==============================] - 1s 41ms/step\n","Iteration 100: Training loss = 1031088.687500\n","32/32 [==============================] - 2s 56ms/step\n","Test loss = 692051.125000\n"]}]},{"cell_type":"code","source":["def create_lstm_encoder_decoder(n_input, n_timesteps, n_features, n_outputs, n_nodes):\n","    inputs = Input(shape=(n_timesteps, n_features))\n","\n","    # encoder layers\n","    encoder = tf.keras.layers.LSTM(n_nodes)(inputs)\n","    encoder = tf.keras.layers.RepeatVector(n_outputs)(encoder)\n","\n","    # decoder layers\n","    decoder = tf.keras.layers.LSTM(n_nodes, return_sequences=True)(encoder)\n","    decoder = tf.keras.layers.TimeDistributed(Dense(n_nodes // 2))(decoder)\n","    decoder = tf.keras.layers.TimeDistributed(Dense(1))(decoder)\n","\n","    model = Model(inputs=inputs, outputs=decoder)\n","    model.compile(loss='mae', optimizer='adam')\n","\n","    return model"],"metadata":{"id":"094aF8pPH1D8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Dense, Dropout\n","from keras.optimizers import Adam\n","import tensorflow_probability as tfp\n","\n","tf.config.experimental_run_functions_eagerly(True)\n","\n","# Define the input and output dimensions\n","input_dim = df.shape[1]\n","output_dim = 1\n","\n","# Define the number of experts\n","num_experts = 3\n","\n","# Define the sizes of the hidden layers for each expert\n","expert_hidden_sizes = [16, 32, 64]\n","\n","# Define the sizes of the output layers for each expert\n","expert_output_sizes = [32, 32, 32]\n","\n","# Define the sizes of the gating network hidden layers\n","gating_hidden_sizes = [16, 8]\n","\n","# Define the size of the output layer of the gating network\n","gating_output_size = num_experts\n","\n","# Define the number of training iterations for the EM algorithm\n","num_iterations = 100\n","\n","# Define the learning rate for the optimization algorithm\n","learning_rate = 0.0001\n","\n","# Load the training data\n","train_data = np.array(df.head(1000))\n","\n","# Split the training data into input and output sequences\n","train_input = train_data[:, :]\n","print('train_input shape', train_input.shape)\n","train_output = train_data[:, -1:]\n","print('train_output shape', train_output.shape)\n","\n","# Define the experts\n","experts = []\n","for i in range(num_experts):\n","    if i != 1:  # Replace the second expert with the LSTM encoder-decoder\n","        expert_input = Input(shape=(input_dim,))\n","        expert_hidden = Dense(expert_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_input)\n","        expert_hidden = Dropout(0.2)(expert_hidden)\n","        expert_output = Dense(expert_output_sizes[i], activation='relu', kernel_initializer='he_normal')(expert_hidden)\n","        expert_output = Dense(output_dim, activation='linear', kernel_initializer='he_normal')(expert_output)\n","        experts.append(Model(inputs=expert_input, outputs=expert_output))\n","    else:\n","        n_input = input_dim  # Modify this value according to the input dimension of your LSTM encoder-decoder\n","        n_timesteps = 1  # Modify this value according to the number of timesteps of your LSTM encoder-decoder\n","        n_features = input_dim  # Modify this value according to the number of features of your LSTM encoder-decoder\n","        n_outputs = 1  # Modify this value according to the number of outputs of your LSTM encoder-decoder\n","        n_nodes = 32  # Modify this value according to the number of nodes of your LSTM encoder-decoder\n","        lstm_expert = create_lstm_encoder_decoder(n_input, n_timesteps, n_features, n_outputs, n_nodes)\n","        experts.append(lstm_expert)\n","\n","# Define the gating network\n","gating_input = Input(shape=(input_dim,))\n","gating_hidden = gating_input\n","for i in range(len(gating_hidden_sizes)):\n","    gating_hidden = Dense(gating_hidden_sizes[i], activation='relu', kernel_initializer='he_normal')(gating_hidden)\n","\n","gating_output = Dense(num_experts + num_experts * 2, activation=None, kernel_initializer='he_normal')(gating_hidden)\n","logits = gating_output[:, :num_experts]\n","params = gating_output[:, num_experts:]\n","params = tf.reshape(params, [-1, num_experts, 2])\n","\n","gating_distribution = tfp.distributions.MixtureSameFamily(\n","    mixture_distribution=tfp.distributions.Categorical(logits=logits),\n","    components_distribution=tfp.distributions.Normal(\n","        loc=params[..., 0],\n","        scale=tf.math.softplus(params[..., 1])\n","    )\n",")\n","\n","gating_model = Model(inputs=gating_input, outputs=logits)\n","\n","# Define the MoE model\n","inputs = Input(shape=(input_dim,))\n","outputs = []\n","for i in range(num_experts):\n","    expert_output = experts[i](inputs)\n","    outputs.append(expert_output)\n","\n","gating_output = gating_model(inputs)\n","weighted_outputs = [tf.expand_dims(gating_output[:, i], axis=-1) * expert_output for i, expert_output in enumerate(outputs)]\n","\n","outputs = tf.reduce_sum(weighted_outputs, axis=0)\n","\n","moe_model = Model(inputs=inputs, outputs=outputs)\n","\n","# Define the loss function\n","def moe_loss(y_true, y_pred):\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    expert_losses = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n","    expert_losses = tf.expand_dims(expert_losses, axis=-1)\n","\n","    # Apply softmax to the logits to get probabilities\n","    gating_probabilities = tf.nn.softmax(gating_output, axis=-1)\n","\n","    # Multiply expert_losses with the gating probabilities instead of logits\n","    gating_losses = tf.reduce_sum(tf.multiply(expert_losses, gating_probabilities), axis=-1)\n","    return tf.reduce_mean(gating_losses)\n","\n","\n","\n","# Define the optimization algorithm\n","optimizer = Adam(lr=learning_rate)\n","\n","# Train the MoE model with the EM algorithm\n","for iteration in range(num_iterations):\n","    # E step: Compute the responsibilities of each expert for each data point\n","    gating_output = tf.constant(gating_model.predict(train_input), dtype=tf.float64)\n","    gating_output /= tf.reduce_sum(gating_output, axis=-1, keepdims=True) + 1e-8  # Add a small epsilon value\n","\n","\n","    # M step: Update the parameters of each expert and the gating network\n","    for i in range(num_experts):\n","        expert_input = train_input\n","        expert_output = experts[i](expert_input)\n","        expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        with tf.GradientTape() as tape:\n","            # Watch the trainable variables of the expert model\n","            tape.watch(experts[i].trainable_variables)\n","\n","            # Define the expert model and calculate the expert_loss\n","            expert_output = experts[i](expert_input)\n","            expert_loss = tf.reduce_mean(tf.square(train_output - expert_output), axis=-1)\n","\n","        # Compute the gradients\n","        expert_gradient = tape.gradient(expert_loss, experts[i].trainable_variables)\n","        # Clip gradients for expert models\n","        expert_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in expert_gradient]\n","\n","        # Update the variables\n","        optimizer.apply_gradients(zip(expert_gradient, experts[i].trainable_variables))\n","\n","\n","    gating_input = train_input\n","\n","    with tf.GradientTape() as tape:\n","      # Watch the trainable variables of the gating model\n","      tape.watch(gating_model.trainable_variables)\n","\n","      # Define the gating model and calculate the gating_loss\n","      gating_output = gating_model(gating_input)\n","      gating_loss = moe_loss(tf.constant(train_output, dtype=tf.float32), moe_model(gating_input))\n","\n","    # Compute the gradients\n","    gating_gradient = tape.gradient(gating_loss, gating_model.trainable_variables)\n","    # Clip gradients for the gating model\n","    gating_gradient = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gating_gradient]\n","\n","    # Update the variables\n","    optimizer.apply_gradients(zip(gating_gradient, gating_model.trainable_variables))\n","\n","\n","    # Evaluate the performance of the MoE model on the training set\n","    train_loss = moe_loss(train_output, moe_model.predict(train_input))\n","    print('Iteration %d: Training loss = %.6f' % (iteration + 1, train_loss))\n","\n","# Make predictions on the test set using the MoE model\n","test_data = np.array(df[-1000:])\n","test_input = test_data[:, :]\n","test_output = test_data[:, -1:]\n","test_predictions = moe_model.predict(test_input)\n","\n","test_loss = moe_loss(test_output, test_predictions)\n","print('Test loss = %.6f' % test_loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"executionInfo":{"status":"error","timestamp":1682203453019,"user_tz":240,"elapsed":820,"user":{"displayName":"eduart andres murcia botache","userId":"18281886472792106858"}},"outputId":"9661e79a-3706-469b-d59c-0ea91a449254","id":"e3pnxlAgH-Bp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_input shape (1000, 2)\n","train_output shape (1000, 1)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-5828667c9109>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_experts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mexpert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"model_51\" (type Functional).\n\nInput 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 2)\n\nCall arguments received by layer \"model_51\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n  • training=None\n  • mask=None"]}]}]}